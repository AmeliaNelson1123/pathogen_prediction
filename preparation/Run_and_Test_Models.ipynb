{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RWKExXYn1xd"
      },
      "source": [
        "# Listeria Model Development and Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction\n",
        "## Goal: \n",
        "Test 7 models (KNN, SVM, GBM, RandomForest, Neural Network, Logistic Regression, Decision Tree) with different data inputs (standardized, original, and with/without cluster and logarithmic columns). Therefore, a grid search method was designed to use hyperparameters to optimize the models, and test on different data inputs.\n",
        "\n",
        "## How to Run\n",
        "Here is a ipynb file that can be run in an IDE or Google Colab for model testing and results. Please input the file name you would like to test, and the predictor column.\n",
        "\n",
        "In this notebook, the exact same models that are provided on the competition, official git repo are used. The code is adapted to perform a grid search (search through a list of hyper parameters) to find the best model results.\n",
        "\n",
        "### To analyze the results, open Analyze_Results.ipynb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rq9x-vqmNpj4"
      },
      "outputs": [],
      "source": [
        "# Importing necessary packages to run the models\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    confusion_matrix,\n",
        "    roc_auc_score,\n",
        "    average_precision_score,\n",
        ")\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Input\n",
        "from pathlib import Path\n",
        "import json\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uM8-xYH7Oy3e"
      },
      "outputs": [],
      "source": [
        "# defining global variables\n",
        "TEST_SIZE = .22  # for validation\n",
        "RANDOM_STATE = 42  # for repeatability\n",
        "# developing results table to plot\n",
        "\n",
        "DATA_PATH = None\n",
        "# !!!!!! IF YOU ARE RUNNING IN VISUAL STUDIO, UNCOMMENT THE CODE BELOW\n",
        "# ROOT = Path.cwd()\n",
        "# if ROOT.name == \"preparation\":\n",
        "#     ROOT = ROOT.parent\n",
        "# DATA = ROOT / \"data\"\n",
        "\n",
        "# getting in file path\n",
        "try:\n",
        "    file_info = Path(DATA_PATH / \"ListeriaSoil_clean_log.csv\")\n",
        "except:\n",
        "    try:\n",
        "        file_info = Path(\"ListeriaSoil_clean_log.csv\")\n",
        "    except Exception as e:\n",
        "        raise e\n",
        "\n",
        "Y_COL = \"binary_listeria_presense\"\n",
        "\n",
        "# changing strings/catagorical data to be encoded in 1-hot vectors\n",
        "# (aka want to transform arbitrary strings into integer values)\n",
        "ENCODE_STR = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQryAxklQMa7"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "T5-KpYLtP6hZ"
      },
      "outputs": [],
      "source": [
        "def data_prep(file_info):\n",
        "    \"\"\"\n",
        "    ----- inputs -----\n",
        "    file_info: Path object\n",
        "        file wanting to process\n",
        "    ----- outputs ----\n",
        "    df: pandas df\n",
        "        processed anonymzied data (string columns representing intervals split into min and max, then put as minimum and maximum values for those columns)\n",
        "    \"\"\"\n",
        "\n",
        "    df = pd.read_csv(Path(file_info.name))\n",
        "\n",
        "    # drop 'index' column if it exists, as it's typically an artifact and not a feature\n",
        "    if 'index' in df.columns:\n",
        "        df = df.drop(columns=['index'])\n",
        "\n",
        "    # converting output column to binary\n",
        "    if Y_COL == \"binary_listeria_presense\":\n",
        "        original_listeria_col = 'Number of Listeria isolates obtained'\n",
        "        df['binary_listeria_presense'] = [row_val if row_val == 0 else 1 for row_val in df[original_listeria_col]]\n",
        "        # Drop the original column to prevent data leakage\n",
        "        if original_listeria_col in df.columns:\n",
        "            df = df.drop(columns=[original_listeria_col])\n",
        "\n",
        "    # switching missing values and weird failures in writing to np.inf bc pandas didnt handle properly\n",
        "    df = df.replace(\"#NAME?\", -np.inf)\n",
        "    df = df.fillna(-np.inf)\n",
        "\n",
        "    # replacing inf with max number that is not max number + 100 in dict (FOR NOT JUST 99999999)\n",
        "    df = df.replace(np.inf, 99999)\n",
        "    # replacing -inf with min number (not -inf) - 100 in dict (FOR NOT JUST -99999999)\n",
        "    df = df.replace(-np.inf, -99999)\n",
        "\n",
        "    # removing any colums that are comletely empty\n",
        "    df = df.dropna(axis=1, how=\"all\")\n",
        "\n",
        "    # applying one-hot encdoing to categorical variables\n",
        "    if ENCODE_STR:\n",
        "        df = pd.get_dummies(df)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQZ0shc5QI89"
      },
      "source": [
        "## Splitting into Train and Test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QT3EkeBZQGOq"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_train_test(\n",
        "    df, y_col=Y_COL, scaling_used=True, test_size=TEST_SIZE,\n",
        "):\n",
        "    \"\"\"\n",
        "    ----- inputs -----\n",
        "    df: pandas dict\n",
        "        processed data (all numerics)\n",
        "    y_col: str\n",
        "        string of y labels\n",
        "    test_size: int\n",
        "        % want test set to be of full data\n",
        "    scaling_used: boolean\n",
        "        whether to test scaled data and original data (True) or only original data (False)\n",
        "    ----- outputs ----\n",
        "    data_testing: dict[str=scalingType][str=y/X train/test label][pd.DataFrame]\n",
        "        dictionary contianing\n",
        "            * string of scaling type (standard scalar, orig)\n",
        "                * string of what dataset grabbing (X_train, X_test, y_train, y_test)\n",
        "                    * corresponding data in a pandas dataframe\n",
        "        \"\n",
        "    \"\"\"\n",
        "\n",
        "    # indexes for test set\n",
        "    X = df.drop(columns=Y_COL)\n",
        "    y = df[Y_COL]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=RANDOM_STATE)\n",
        "\n",
        "    # data columns\n",
        "    data_columns = X.columns\n",
        "\n",
        "    if scaling_used:  # if want to run on scaled and original data\n",
        "        # testing all with and without scaled data\n",
        "        scaler = StandardScaler()\n",
        "\n",
        "        X_train_scaled = scaler.fit_transform(X_train.values)\n",
        "        X_test_scaled = scaler.transform(X_test.values)\n",
        "\n",
        "        data_testing = {\n",
        "            \"columns\": data_columns,\n",
        "            \"standard_scalar\": {\n",
        "                \"X_train\": X_train_scaled,\n",
        "                \"X_test\": X_test_scaled,\n",
        "                \"y_train\": y_train,  # using unscaled y\n",
        "                \"y_test\": y_test,  # using unscaled y\n",
        "            },\n",
        "            \"orig\": {\n",
        "                \"X_train\": X_train,\n",
        "                \"X_test\": X_test,\n",
        "                \"y_train\": y_train,\n",
        "                \"y_test\": y_test,\n",
        "            },\n",
        "        }\n",
        "\n",
        "        return data_testing\n",
        "\n",
        "    else:  # if only want to run on original data\n",
        "        data_testing = {\n",
        "            \"columns\": data_columns,\n",
        "            \"orig\": {\n",
        "                \"X_train\": X_train,\n",
        "                \"X_test\": X_test,\n",
        "                \"y_train\": y_train,\n",
        "                \"y_test\": y_test,\n",
        "            }\n",
        "        }\n",
        "        return data_testing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atQqkgpGQjFV"
      },
      "source": [
        "# Model Development"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fLc6J0NFRamd"
      },
      "outputs": [],
      "source": [
        "def test_svm(data_testing, file_info, model_predictions_data):\n",
        "    \"\"\"\n",
        "    ----- inputs -----\n",
        "    data_testing: dict[str=scalingType][str=y/X train/test label][pd.DataFrame]\n",
        "        dictionary contianing\n",
        "            * string of scaling type (standard scalar, orig)\n",
        "                * string of what dataset grabbing (X_train, X_test, y_train, y_test)\n",
        "                    * corresponding data in a pandas dataframe\n",
        "    model_predictions_data: list\n",
        "        A list to store dictionaries of y_true and y_pred_proba for PR/AUC curves.\n",
        "    ----- outputs ----\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # results table\n",
        "    svm_results = []\n",
        "\n",
        "    # defining hyperparameters for svm variables\n",
        "    c_vals = [1, 4]\n",
        "    svm_kernels = ['linear', 'rbf']\n",
        "\n",
        "    # grid searching model results for svm on all types of data with all types of inputs\n",
        "    for scalar_type in tqdm(data_testing.keys(), desc=\"svm scaled vs original\"):\n",
        "        if scalar_type == 'columns':\n",
        "            continue\n",
        "        X_test = data_testing[scalar_type][\"X_test\"]\n",
        "        X_train = data_testing[scalar_type][\"X_train\"]\n",
        "        y_train = data_testing[scalar_type][\"y_train\"]\n",
        "        y_test = data_testing[scalar_type][\"y_test\"]\n",
        "        feature_names = data_testing[\"columns\"].tolist()\n",
        "\n",
        "        # going through possible svm combos\n",
        "        for c_val in c_vals:\n",
        "            for svm_kernel in svm_kernels:\n",
        "                # modeling portion\n",
        "                model = SVC(C=c_val, kernel=svm_kernel, max_iter=20000, probability=True)\n",
        "                model.fit(X_train, y_train)\n",
        "                y_pred = model.predict(X_test)\n",
        "                y_pred_proba = model.predict_proba(X_test)[:, 1] # Probability of the positive class\n",
        "\n",
        "                # validation\n",
        "                accuracy = accuracy_score(y_test, y_pred)\n",
        "                precision = precision_score(y_test, y_pred, zero_division=0)\n",
        "                recall = recall_score(y_test, y_pred, zero_division=0)\n",
        "                f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "                conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "                roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "                pr_auc = average_precision_score(y_test, y_pred_proba)\n",
        "\n",
        "                # getting feature importance\n",
        "                coefficients = np.nan\n",
        "                feature_imp = np.nan\n",
        "                feature_imp_json = np.nan\n",
        "                if svm_kernel == 'linear':\n",
        "                    coefficients = model.coef_.ravel()\n",
        "                    feature_imp = dict(zip(feature_names, coefficients))\n",
        "                    feature_imp_json = json.dumps({k: float(v) for k, v in feature_imp.items()})\n",
        "\n",
        "                # getting permutation importance\n",
        "                perm = permutation_importance(\n",
        "                    model, X_test, y_test,\n",
        "                    n_repeats=3,\n",
        "                    random_state=RANDOM_STATE,\n",
        "                    scoring=\"f1\"  # or \"accuracy\"\n",
        "                )\n",
        "\n",
        "                perm_imp = dict(zip(feature_names, perm.importances_mean))\n",
        "                perm_imp_json = json.dumps({k: float(v) for k, v in perm_imp.items()})\n",
        "\n",
        "                # adding hyperparameters to each of these results/outputs: saving results to dict\n",
        "                svm_results.append(\n",
        "                    {\n",
        "                        \"file name\": file_info.name,\n",
        "                        \"accuracy\": accuracy,\n",
        "                        \"precision\": precision,\n",
        "                        \"recall\": recall,\n",
        "                        \"f1\": f1,\n",
        "                        \"roc_auc\": roc_auc,\n",
        "                        \"pr_auc\": pr_auc,\n",
        "                        \"confusion matrix\": conf_matrix,\n",
        "                        \"test size\": TEST_SIZE,\n",
        "                        \"random state\": RANDOM_STATE,\n",
        "                        \"scalar_status\": scalar_type,\n",
        "                        \"y variable used\": Y_COL,\n",
        "                        \"model used\": \"svm\",\n",
        "                        \"logistic_reg_c\": np.nan,\n",
        "                        \"lr_ratios\": np.nan,\n",
        "                        \"nn_layers\": np.nan,\n",
        "                        \"nn_neurons\": np.nan,\n",
        "                        \"nn_batch_size\": np.nan,\n",
        "                        \"nn_epochs\": np.nan,\n",
        "                        \"dt_max_depth\": np.nan,\n",
        "                        \"dt_min_samples_split\": np.nan,\n",
        "                        \"svm_c_val\": c_val,\n",
        "                        \"svm_kernel\": svm_kernel,\n",
        "                        \"knn_weights\": np.nan,\n",
        "                        \"gbm_learning_rate\": np.nan,\n",
        "                        \"gbm_n_estimator\": np.nan,\n",
        "                        \"rf_n_estimators\": np.nan,\n",
        "                        \"rf_max_depth\": np.nan,\n",
        "                        \"rf_min_samples_leaf\": np.nan,\n",
        "                        \"coefficient_importance\": feature_imp_json,\n",
        "                        \"permutation_importance\": perm_imp_json,\n",
        "                        \"y_test\": y_test.tolist(),\n",
        "                        \"y_pred_proba\": y_pred_proba.tolist(),\n",
        "                    }\n",
        "                )\n",
        "\n",
        "                # store y_test and y_pred_proba for PR/AUC plotting\n",
        "                model_predictions_data.append({\n",
        "                    \"file name\": file_info.name,\n",
        "                    \"model used\": \"svm\",\n",
        "                    \"scalar_status\": scalar_type,\n",
        "                    \"svm_c_val\": c_val,\n",
        "                    \"svm_kernel\": svm_kernel,\n",
        "                    \"y_test\": y_test.tolist(),\n",
        "                    \"y_pred_proba\": y_pred_proba.tolist()\n",
        "                })\n",
        "\n",
        "    return svm_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RveNGUPxQnqc"
      },
      "outputs": [],
      "source": [
        "def test_logistic_reg(data_testing, file_info, model_predictions_data):\n",
        "    \"\"\"\n",
        "    ----- inputs -----\n",
        "    data_testing: dict[str=scalingType][str=y/X train/test label][pd.DataFrame]\n",
        "        dictionary contianing\n",
        "            * string of scaling type (standard scalar, orig)\n",
        "                * string of what dataset grabbing (X_train, X_test, y_train, y_test)\n",
        "                    * corresponding data in a pandas dataframe\n",
        "    model_predictions_data: list\n",
        "        A list to store dictionaries of y_true and y_pred_proba for PR/AUC curves.\n",
        "    ----- outputs ----\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    log_reg_results = []\n",
        "\n",
        "    # defining hyperparameters for logistic regression variables\n",
        "    c_vals = [0.01, 0.1, 1, 4, 8]\n",
        "    lr_ratios = [\n",
        "        0,\n",
        "        1,\n",
        "    ]  # 0 = l2 penalty, 1 = l1 penalty\n",
        "\n",
        "    # grid searching model results for log reg on all types of data with all types of inputs\n",
        "    for scalar_type in tqdm(data_testing.keys(), desc=\"logistic regression scaled vs original\"):\n",
        "\n",
        "        if scalar_type == 'columns':\n",
        "            continue\n",
        "        X_test = data_testing[scalar_type][\"X_test\"]\n",
        "        X_train = data_testing[scalar_type][\"X_train\"]\n",
        "        y_train = data_testing[scalar_type][\"y_train\"]\n",
        "        y_test = data_testing[scalar_type][\"y_test\"]\n",
        "        feature_names = data_testing[\"columns\"].tolist()\n",
        "\n",
        "        # going through possible logistic regression combos\n",
        "        for c_val in c_vals:\n",
        "            for lr_rat in lr_ratios:\n",
        "                # modeling portion\n",
        "                model = LogisticRegression(C=c_val, l1_ratio=lr_rat)\n",
        "                model.fit(X_train, y_train)\n",
        "                y_pred = model.predict(X_test)\n",
        "                y_pred_proba = model.predict_proba(X_test)[:, 1] # Probability of the positive class\n",
        "\n",
        "                # validation\n",
        "                accuracy = accuracy_score(y_test, y_pred)\n",
        "                precision = precision_score(y_test, y_pred, zero_division=0)\n",
        "                recall = recall_score(y_test, y_pred, zero_division=0)\n",
        "                f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "                conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "                roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "                pr_auc = average_precision_score(y_test, y_pred_proba)\n",
        "\n",
        "                # getting feature importance\n",
        "                coefficients = model.coef_.ravel()\n",
        "                feature_imp = dict(zip(feature_names, coefficients))\n",
        "                feature_imp_json = json.dumps({k: float(v) for k, v in feature_imp.items()})\n",
        "\n",
        "                # getting permutation importance\n",
        "                perm = permutation_importance(\n",
        "                    model, X_test, y_test,\n",
        "                    n_repeats=10,\n",
        "                    random_state=RANDOM_STATE,\n",
        "                    scoring=\"f1\"  # or \"accuracy\"\n",
        "                )\n",
        "\n",
        "                perm_imp = dict(zip(feature_names, perm.importances_mean))\n",
        "                perm_imp_json = json.dumps({k: float(v) for k, v in perm_imp.items()})\n",
        "\n",
        "                # adding hyperparameters to each of these results/outputs: saving results to dictfile_path: str\n",
        "                log_reg_results.append(\n",
        "                    {\n",
        "                        \"file name\": file_info.name,\n",
        "                        \"accuracy\": accuracy,\n",
        "                        \"precision\": precision,\n",
        "                        \"recall\": recall,\n",
        "                        \"f1\": f1,\n",
        "                        \"roc_auc\": roc_auc,\n",
        "                        \"pr_auc\": pr_auc,\n",
        "                        \"confusion matrix\": conf_matrix,\n",
        "                        \"test size\": TEST_SIZE,\n",
        "                        \"random state\": RANDOM_STATE,\n",
        "                        \"scalar_status\": scalar_type,\n",
        "                        \"y variable used\": Y_COL,\n",
        "                        \"model used\": \"logistic regression\",\n",
        "                        \"logistic_reg_c\": c_val,\n",
        "                        \"lr_ratios\": lr_rat,\n",
        "                        \"nn_layers\": np.nan,\n",
        "                        \"nn_neurons\": np.nan,\n",
        "                        \"nn_batch_size\": np.nan,\n",
        "                        \"nn_epochs\": np.nan,\n",
        "                        \"dt_max_depth\": np.nan,\n",
        "                        \"dt_min_samples_split\": np.nan,\n",
        "                        \"svm_c_val\": np.nan,\n",
        "                        \"svm_kernel\": np.nan,\n",
        "                        \"knn_n_neighbor\": np.nan,\n",
        "                        \"knn_weights\": np.nan,\n",
        "                        \"gbm_learning_rate\": np.nan,\n",
        "                        \"gbm_n_estimator\": np.nan,\n",
        "                        \"rf_n_estimators\": np.nan,\n",
        "                        \"rf_max_depth\": np.nan,\n",
        "                        \"rf_min_samples_leaf\": np.nan,\n",
        "                        \"coefficient_importance\": feature_imp_json,\n",
        "                        \"permutation_importance\": perm_imp_json,\n",
        "                        \"y_test\": y_test.tolist(), # Added y_test to results\n",
        "                        \"y_pred_proba\": y_pred_proba.tolist(),\n",
        "                    }\n",
        "                )\n",
        "\n",
        "                # Store y_test and y_pred_proba for PR/AUC plotting\n",
        "                model_predictions_data.append({\n",
        "                    \"file name\": file_info.name,\n",
        "                    \"model used\": \"logistic regression\",\n",
        "                    \"scalar_status\": scalar_type,\n",
        "                    \"logistic_reg_c\": c_val,\n",
        "                    \"lr_ratios\": lr_rat,\n",
        "                    \"y_test\": y_test.tolist(),\n",
        "                    \"y_pred_proba\": y_pred_proba.tolist()\n",
        "                })\n",
        "\n",
        "    return log_reg_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "T7iYfaUXRhX-"
      },
      "outputs": [],
      "source": [
        "def test_knn(data_testing, file_info, model_predictions_data):\n",
        "    \"\"\"\n",
        "    ----- inputs -----\n",
        "    data_testing: dict[str=scalingType][str=y/X train/test label][pd.DataFrame]\n",
        "        dictionary contianing\n",
        "            * string of scaling type (standard scalar, orig)\n",
        "                * string of what dataset grabbing (X_train, X_test, y_train, y_test)\n",
        "                    * corresponding data in a pandas dataframe\n",
        "    model_predictions_data: list\n",
        "        A list to store dictionaries of y_true and y_pred_proba for PR/AUC curves.\n",
        "    ----- outputs ----\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # results table\n",
        "    knn_results = []\n",
        "\n",
        "    # defining hyperparameters for knn variables\n",
        "    knn_n_neighbors = [2, 5, 10, 15, 20]\n",
        "    weights = ['uniform', 'distance']\n",
        "\n",
        "    # grid searching model results for knn on all types of data with all types of inputs\n",
        "    for scalar_type in tqdm(data_testing.keys(), desc=\"knn scaled vs original\"):\n",
        "        if scalar_type == 'columns':\n",
        "            continue\n",
        "        X_test = data_testing[scalar_type][\"X_test\"]\n",
        "        X_train = data_testing[scalar_type][\"X_train\"]\n",
        "        y_train = data_testing[scalar_type][\"y_train\"]\n",
        "        y_test = data_testing[scalar_type][\"y_test\"]\n",
        "        feature_names = data_testing[\"columns\"].tolist()\n",
        "\n",
        "        # going through possible KNN combos\n",
        "        for knn_n_neighbor in knn_n_neighbors:\n",
        "            for weight in weights:\n",
        "                # modeling portion\n",
        "                model = KNeighborsClassifier(\n",
        "                    n_neighbors=knn_n_neighbor, weights=weight\n",
        "                )\n",
        "                model.fit(X_train, y_train)\n",
        "                y_pred = model.predict(X_test)\n",
        "                y_pred_proba = model.predict_proba(X_test)[:, 1] # Probability of the positive class\n",
        "\n",
        "                # validation\n",
        "                accuracy = accuracy_score(y_test, y_pred)\n",
        "                precision = precision_score(y_test, y_pred, zero_division=0)\n",
        "                recall = recall_score(y_test, y_pred, zero_division=0)\n",
        "                f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "                conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "                roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "                pr_auc = average_precision_score(y_test, y_pred_proba)\n",
        "\n",
        "                # getting permutation importance\n",
        "                perm = permutation_importance(\n",
        "                    model, X_test, y_test,\n",
        "                    n_repeats=3, # reduced number to speed up model runtime\n",
        "                    random_state=RANDOM_STATE,\n",
        "                    scoring=\"f1\"  # or \"accuracy\"\n",
        "                )\n",
        "\n",
        "                perm_imp = dict(zip(feature_names, perm.importances_mean))\n",
        "                perm_imp_json = json.dumps({k: float(v) for k, v in perm_imp.items()})\n",
        "\n",
        "                # adding hyperparameters to each of these results/outputs: saving results to dict\n",
        "                knn_results.append(\n",
        "                    {\n",
        "                        \"file name\": file_info.name,\n",
        "                        \"accuracy\": accuracy,\n",
        "                        \"precision\": precision,\n",
        "                        \"recall\": recall,\n",
        "                        \"f1\": f1,\n",
        "                        \"roc_auc\": roc_auc,\n",
        "                        \"pr_auc\": pr_auc,\n",
        "                        \"confusion matrix\": conf_matrix,\n",
        "                        \"test size\": TEST_SIZE,\n",
        "                        \"random state\": RANDOM_STATE,\n",
        "                        \"scalar_status\": scalar_type,\n",
        "                        \"y variable used\": Y_COL,\n",
        "                        \"model used\": \"knn\",\n",
        "                        \"logistic_reg_c\": np.nan,\n",
        "                        \"lr_ratios\": np.nan,\n",
        "                        \"nn_layers\": np.nan,\n",
        "                        \"nn_neurons\": np.nan,\n",
        "                        \"nn_batch_size\": np.nan,\n",
        "                        \"nn_epochs\": np.nan,\n",
        "                        \"dt_max_depth\": np.nan,\n",
        "                        \"dt_min_samples_split\": np.nan,\n",
        "                        \"svm_c_val\": np.nan,\n",
        "                        \"svm_kernel\": np.nan,\n",
        "                        \"knn_n_neighbor\": knn_n_neighbor,\n",
        "                        \"knn_weights\": weight,\n",
        "                        \"gbm_learning_rate\": np.nan,\n",
        "                        \"gbm_n_estimator\": np.nan,\n",
        "                        \"rf_n_estimators\": np.nan,\n",
        "                        \"rf_max_depth\": np.nan,\n",
        "                        \"rf_min_samples_leaf\": np.nan,\n",
        "                        \"coefficient_importance\": np.nan,\n",
        "                        \"permutation_importance\": perm_imp_json,\n",
        "                        \"y_test\": y_test.tolist(), # Added y_test to results\n",
        "                        \"y_pred_proba\": y_pred_proba.tolist(),\n",
        "                    }\n",
        "                )\n",
        "\n",
        "                # Store y_test and y_pred_proba for PR/AUC plotting\n",
        "                model_predictions_data.append({\n",
        "                    \"file name\": file_info.name,\n",
        "                    \"model used\": \"knn\",\n",
        "                    \"scalar_status\": scalar_type,\n",
        "                    \"knn_n_neighbor\": knn_n_neighbor,\n",
        "                    \"knn_weights\": weight,\n",
        "                    \"y_test\": y_test.tolist(),\n",
        "                    \"y_pred_proba\": y_pred_proba.tolist()\n",
        "                })\n",
        "\n",
        "    return knn_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ckfrxeJEQyb1"
      },
      "outputs": [],
      "source": [
        "def test_neural_net(data_testing, file_info, model_predictions_data):\n",
        "    \"\"\"\n",
        "    ----- inputs -----\n",
        "    data_testing: dict[str=scalingType][str=y/X train/test label][pd.DataFrame]\n",
        "        dictionary contianing\n",
        "            * string of scaling type (standard scalar, orig)\n",
        "                * string of what dataset grabbing (X_train, X_test, y_train, y_test)\n",
        "                    * corresponding data in a pandas dataframe\n",
        "    model_predictions_data: list\n",
        "        A list to store dictionaries of y_true and y_pred_proba for PR/AUC curves.\n",
        "    ----- outputs ----\n",
        "\n",
        "    \"\"\"\n",
        "    # results table\n",
        "    neur_net_results = []\n",
        "\n",
        "    # editing hyperparameters for neural network variables\n",
        "    nn_layers_list = [1, 2, 3, 4]\n",
        "    nn_neurons_list = [16, 32, 64, 128, 256]\n",
        "    nn_batch_size_list = [32, 64, 128, 256]\n",
        "    nn_epochs_list = [5, 10, 20]\n",
        "\n",
        "    # grid searching model results for neural net on all types of data with all types of inputs\n",
        "    for scalar_type in tqdm(data_testing.keys(), desc=\"neural net scaled vs original\"):\n",
        "        if scalar_type == 'columns':\n",
        "            continue\n",
        "        X_test = data_testing[scalar_type][\"X_test\"]\n",
        "        X_train = data_testing[scalar_type][\"X_train\"]\n",
        "        y_train = data_testing[scalar_type][\"y_train\"]\n",
        "        y_test = data_testing[scalar_type][\"y_test\"]\n",
        "\n",
        "        # going through possible neural net combos\n",
        "        for nn_layers in nn_layers_list:\n",
        "            for nn_neurons in nn_neurons_list:\n",
        "                for nn_batch_size in nn_batch_size_list:\n",
        "                    for nn_epochs in nn_epochs_list:\n",
        "                        # modeling portion\n",
        "                        model = Sequential()\n",
        "                        model.add(Input(shape=(X_train.shape[1],)))\n",
        "                        for _ in range(nn_layers):\n",
        "                            model.add(Dense(nn_neurons, activation=\"relu\"))\n",
        "                        model.add(Dense(1, activation=\"sigmoid\"))\n",
        "                        model.compile(\n",
        "                            optimizer=\"adam\",\n",
        "                            loss=\"binary_crossentropy\",\n",
        "                            metrics=[\"accuracy\"],\n",
        "                        )\n",
        "                        model.fit(\n",
        "                            X_train,\n",
        "                            y_train,\n",
        "                            epochs=nn_epochs,\n",
        "                            batch_size=nn_batch_size,\n",
        "                            verbose=0,\n",
        "                        )\n",
        "                        y_pred_proba = model.predict(X_test).flatten()\n",
        "                        y_pred = (y_pred_proba > 0.5).astype(int)\n",
        "\n",
        "                        # validation\n",
        "                        accuracy = accuracy_score(y_test, y_pred)\n",
        "                        precision = precision_score(y_test, y_pred, zero_division=0)\n",
        "                        recall = recall_score(y_test, y_pred, zero_division=0)\n",
        "                        f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "                        conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "                        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "                        pr_auc = average_precision_score(y_test, y_pred_proba)\n",
        "\n",
        "                        # adding hyperparameters to each of these results/outputs: saving results to dict\n",
        "                        neur_net_results.append(\n",
        "                            {\n",
        "                                \"file name\": file_info.name,\n",
        "                                \"accuracy\": accuracy,\n",
        "                                \"precision\": precision,\n",
        "                                \"recall\": recall,\n",
        "                                \"f1\": f1,\n",
        "                                \"roc_auc\": roc_auc,\n",
        "                                \"pr_auc\": pr_auc,\n",
        "                                \"confusion matrix\": conf_matrix,\n",
        "                                \"test size\": TEST_SIZE,\n",
        "                                \"random state\": RANDOM_STATE,\n",
        "                                \"scalar_status\": scalar_type,\n",
        "                                \"y variable used\": Y_COL,\n",
        "                                \"model used\": \"neural net\",\n",
        "                                \"logistic_reg_c\": np.nan,\n",
        "                                \"lr_ratios\": np.nan,\n",
        "                                \"nn_layers\": nn_layers,\n",
        "                                \"nn_neurons\": nn_neurons,\n",
        "                                \"nn_batch_size\": nn_batch_size,\n",
        "                                \"nn_epochs\": nn_epochs,\n",
        "                                \"dt_max_depth\": np.nan,\n",
        "                                \"dt_min_samples_split\": np.nan,\n",
        "                                \"svm_c_val\": np.nan,\n",
        "                                \"svm_kernel\": np.nan,\n",
        "                                \"knn_n_neighbor\": np.nan,\n",
        "                                \"knn_weights\": np.nan,\n",
        "                                \"gbm_learning_rate\": np.nan,\n",
        "                                \"gbm_n_estimator\": np.nan,\n",
        "                                \"rf_n_estimators\": np.nan,\n",
        "                                \"rf_max_depth\": np.nan,\n",
        "                                \"rf_min_samples_leaf\": np.nan,\n",
        "                                \"coefficient_importance\": np.nan,\n",
        "                                \"permutation_importance\": np.nan,\n",
        "                                \"y_test\": y_test.tolist(), # Added y_test to results\n",
        "                                \"y_pred_proba\": y_pred_proba.tolist(),\n",
        "                            }\n",
        "                        )\n",
        "\n",
        "                        # Store y_test and y_pred_proba for PR/AUC plotting\n",
        "                        model_predictions_data.append({\n",
        "                            \"file name\": file_info.name,\n",
        "                            \"model used\": \"neural net\",\n",
        "                            \"scalar_status\": scalar_type,\n",
        "                            \"nn_layers\": nn_layers,\n",
        "                            \"nn_neurons\": nn_neurons,\n",
        "                            \"nn_batch_size\": nn_batch_size,\n",
        "                            \"nn_epochs\": nn_epochs,\n",
        "                            \"y_test\": y_test.tolist(),\n",
        "                            \"y_pred_proba\": y_pred_proba.tolist()\n",
        "                        })\n",
        "\n",
        "    return neur_net_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DmdtiU0wSBSY"
      },
      "outputs": [],
      "source": [
        "def test_gbm(data_testing, file_info, model_predictions_data):\n",
        "    \"\"\"\n",
        "    ----- inputs -----\n",
        "    data_testing: dict[str=scalingType][str=y/X train/test label][pd.DataFrame]\n",
        "        dictionary contianing\n",
        "            * string of scaling type (standard scalar, orig)\n",
        "                * string of what dataset grabbing (X_train, X_test, y_train, y_test)\n",
        "                    * corresponding data in a pandas dataframe\n",
        "    model_predictions_data: list\n",
        "        A list to store dictionaries of y_true and y_pred_proba for PR/AUC curves.\n",
        "    ----- outputs ----\n",
        "\n",
        "    \"\"\"\n",
        "    # results table\n",
        "    gbm_results = []\n",
        "\n",
        "    #  editing hyperparameters for gbm variables\n",
        "    gbm_learning_rates = [0.01, 0.05, 0.1, 0.2]\n",
        "    gbm_n_estimators = [100, 200, 400, 800]\n",
        "\n",
        "    # grid searching model results for gbm on all types of data with all types of inputs\n",
        "    for scalar_type in tqdm(data_testing.keys(), desc=\"gbm scaled vs original\"):\n",
        "        if scalar_type == 'columns':\n",
        "            continue\n",
        "        X_test = data_testing[scalar_type][\"X_test\"]\n",
        "        X_train = data_testing[scalar_type][\"X_train\"]\n",
        "        y_train = data_testing[scalar_type][\"y_train\"]\n",
        "        y_test = data_testing[scalar_type][\"y_test\"]\n",
        "        feature_names = data_testing[\"columns\"].tolist()\n",
        "\n",
        "        # going through possible gbm combos\n",
        "        for gbm_learning_rate in gbm_learning_rates:\n",
        "            for gbm_n_estimator in gbm_n_estimators:\n",
        "                # modeling portion\n",
        "                model = GradientBoostingClassifier(\n",
        "                    learning_rate=gbm_learning_rate, n_estimators=gbm_n_estimator\n",
        "                )\n",
        "                model.fit(X_train, y_train)\n",
        "                y_pred = model.predict(X_test)\n",
        "                y_pred_proba = model.predict_proba(X_test)[:, 1] # Probability of the positive class\n",
        "\n",
        "                # validation\n",
        "                accuracy = accuracy_score(y_test, y_pred)\n",
        "                precision = precision_score(y_test, y_pred, zero_division=0)\n",
        "                recall = recall_score(y_test, y_pred, zero_division=0)\n",
        "                f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "                conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "                roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "                pr_auc = average_precision_score(y_test, y_pred_proba)\n",
        "\n",
        "                # getting feature importance\n",
        "                gbm_imp = model.feature_importances_\n",
        "                feature_imp = dict(zip(feature_names, gbm_imp))\n",
        "                feature_imp_json = json.dumps({k: float(v) for k, v in feature_imp.items()})\n",
        "\n",
        "                # getting permutation importance\n",
        "                perm = permutation_importance(\n",
        "                    model, X_test, y_test,\n",
        "                    n_repeats=10,\n",
        "                    random_state=RANDOM_STATE,\n",
        "                    scoring=\"f1\"  # or \"accuracy\"\n",
        "                )\n",
        "\n",
        "                perm_imp = dict(zip(feature_names, perm.importances_mean))\n",
        "                perm_imp_json = json.dumps({k: float(v) for k, v in perm_imp.items()})\n",
        "\n",
        "                # adding hyperparameters to each of these results/outputs: saving results to dict\n",
        "                gbm_results.append(\n",
        "                    {\n",
        "                        \"file name\": file_info.name,\n",
        "                        \"accuracy\": accuracy,\n",
        "                        \"precision\": precision,\n",
        "                        \"recall\": recall,\n",
        "                        \"f1\": f1,\n",
        "                        \"roc_auc\": roc_auc,\n",
        "                        \"pr_auc\": pr_auc,\n",
        "                        \"confusion matrix\": conf_matrix,\n",
        "                        \"test size\": TEST_SIZE,\n",
        "                        \"random state\": RANDOM_STATE,\n",
        "                        \"scalar_status\": scalar_type,\n",
        "                        \"y variable used\": Y_COL,\n",
        "                        \"model used\": \"gbm\",\n",
        "                        \"logistic_reg_c\": np.nan,\n",
        "                        \"lr_ratios\": np.nan,\n",
        "                        \"nn_layers\": np.nan,\n",
        "                        \"nn_neurons\": np.nan,\n",
        "                        \"nn_batch_size\": np.nan,\n",
        "                        \"nn_epochs\": np.nan,\n",
        "                        \"dt_max_depth\": np.nan,\n",
        "                        \"dt_min_samples_split\": np.nan,\n",
        "                        \"svm_c_val\": np.nan,\n",
        "                        \"svm_kernel\": np.nan,\n",
        "                        \"knn_n_neighbor\": np.nan,\n",
        "                        \"knn_weights\": np.nan,\n",
        "                        \"gbm_learning_rate\": gbm_learning_rate,\n",
        "                        \"gbm_n_estimator\": gbm_n_estimator,\n",
        "                        \"rf_n_estimators\": np.nan,\n",
        "                        \"rf_max_depth\": np.nan,\n",
        "                        \"rf_min_samples_leaf\": np.nan,\n",
        "                        \"coefficient_importance\": feature_imp_json,\n",
        "                        \"permutation_importance\": perm_imp_json,\n",
        "                        \"y_test\": y_test.tolist(), # Added y_test to results\n",
        "                        \"y_pred_proba\": y_pred_proba.tolist(),\n",
        "                    }\n",
        "                )\n",
        "\n",
        "                # Store y_test and y_pred_proba for PR/AUC plotting\n",
        "                model_predictions_data.append({\n",
        "                    \"file name\": file_info.name,\n",
        "                    \"model used\": \"gbm\",\n",
        "                    \"scalar_status\": scalar_type,\n",
        "                    \"gbm_learning_rate\": gbm_learning_rate,\n",
        "                    \"gbm_n_estimator\": gbm_n_estimator,\n",
        "                    \"y_test\": y_test.tolist(),\n",
        "                    \"y_pred_proba\": y_pred_proba.tolist()\n",
        "                })\n",
        "    return gbm_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "zEMr1gIvQ8SJ"
      },
      "outputs": [],
      "source": [
        "def test_decision_tree(data_testing, file_info, model_predictions_data):\n",
        "    \"\"\"\n",
        "    ----- inputs -----\n",
        "    data_testing: dict[str=scalingType][str=y/X train/test label][pd.DataFrame]\n",
        "        dictionary contianing\n",
        "            * string of scaling type (standard scalar, orig)\n",
        "                * string of what dataset grabbing (X_train, X_test, y_train, y_test)\n",
        "                    * corresponding data in a pandas dataframe\n",
        "    model_predictions_data: list\n",
        "        A list to store dictionaries of y_true and y_pred_proba for PR/AUC curves.\n",
        "    ----- outputs ----\n",
        "\n",
        "    \"\"\"\n",
        "    # results table\n",
        "    dec_tree_results = []\n",
        "\n",
        "    #  editing hyperparameters for decision tree variables\n",
        "    dt_max_depths = [50, 100, 200, 400, None]\n",
        "    dt_min_samples_splits = [2, 10, 20, 50]\n",
        "\n",
        "    # grid searching model results for decision tree on all types of data with all types of inputs\n",
        "    for scalar_type in tqdm(data_testing.keys(), desc=\"decision tree scaled vs original\"):\n",
        "        if scalar_type == 'columns':\n",
        "            continue\n",
        "        X_test = data_testing[scalar_type][\"X_test\"]\n",
        "        X_train = data_testing[scalar_type][\"X_train\"]\n",
        "        y_train = data_testing[scalar_type][\"y_train\"]\n",
        "        y_test = data_testing[scalar_type][\"y_test\"]\n",
        "        feature_names = data_testing[\"columns\"].tolist()\n",
        "\n",
        "        # going through possible decision tree combos\n",
        "        for dt_min_samples_split in dt_min_samples_splits:\n",
        "            for dt_max_depth in dt_max_depths:\n",
        "                # modeling portion\n",
        "                model = DecisionTreeClassifier(\n",
        "                    max_depth=dt_max_depth, min_samples_split=dt_min_samples_split\n",
        "                )\n",
        "                model.fit(X_train, y_train)\n",
        "                y_pred = model.predict(X_test)\n",
        "                y_pred_proba = model.predict_proba(X_test)[:, 1] # Probability of the positive class\n",
        "\n",
        "                # validation\n",
        "                accuracy = accuracy_score(y_test, y_pred)\n",
        "                precision = precision_score(y_test, y_pred, zero_division=0)\n",
        "                recall = recall_score(y_test, y_pred, zero_division=0)\n",
        "                f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "                conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "                roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "                pr_auc = average_precision_score(y_test, y_pred_proba)\n",
        "\n",
        "                # getting feature importance\n",
        "                tree_imp = model.feature_importances_\n",
        "                feature_imp = dict(zip(feature_names, tree_imp))\n",
        "                feature_imp_json = json.dumps({k: float(v) for k, v in feature_imp.items()})\n",
        "\n",
        "                # getting permutation importance\n",
        "                perm = permutation_importance(\n",
        "                    model, X_test, y_test,\n",
        "                    n_repeats=10,\n",
        "                    random_state=RANDOM_STATE,\n",
        "                    scoring=\"f1\"  # or \"accuracy\"\n",
        "                )\n",
        "\n",
        "                perm_imp = dict(zip(feature_names, perm.importances_mean))\n",
        "                perm_imp_json = json.dumps({k: float(v) for k, v in perm_imp.items()})\n",
        "\n",
        "                # adding hyperparameters to each of these results/outputs\n",
        "                dec_tree_results.append(\n",
        "                    {\n",
        "                        \"file name\": file_info.name,\n",
        "                        \"accuracy\": accuracy,\n",
        "                        \"precision\": precision,\n",
        "                        \"recall\": recall,\n",
        "                        \"f1\": f1,\n",
        "                        \"roc_auc\": roc_auc,\n",
        "                        \"pr_auc\": pr_auc,\n",
        "                        \"confusion matrix\": conf_matrix,\n",
        "                        \"test size\": TEST_SIZE,\n",
        "                        \"random state\": RANDOM_STATE,\n",
        "                        \"scalar_status\": scalar_type,\n",
        "                        \"y variable used\": Y_COL,\n",
        "                        \"model used\": \"decision_tree\",\n",
        "                        \"logistic_reg_c\": np.nan,\n",
        "                        \"lr_ratios\": np.nan,\n",
        "                        \"nn_layers\": np.nan,\n",
        "                        \"nn_neurons\": np.nan,\n",
        "                        \"nn_batch_size\": np.nan,\n",
        "                        \"nn_epochs\": np.nan,\n",
        "                        \"dt_max_depth\": dt_max_depth,\n",
        "                        \"dt_min_samples_split\": dt_min_samples_split,\n",
        "                        \"svm_c_val\": np.nan,\n",
        "                        \"svm_kernel\": np.nan,\n",
        "                        \"knn_n_neighbor\": np.nan,\n",
        "                        \"knn_weights\": np.nan,\n",
        "                        \"gbm_learning_rate\": np.nan,\n",
        "                        \"gbm_n_estimator\": np.nan,\n",
        "                        \"rf_n_estimators\": np.nan,\n",
        "                        \"rf_max_depth\": np.nan,\n",
        "                        \"rf_min_samples_leaf\": np.nan,\n",
        "                        \"coefficient_importance\": feature_imp_json,\n",
        "                        \"permutation_importance\": perm_imp_json,\n",
        "                        \"y_test\": y_test.tolist(), # Added y_test to results\n",
        "                        \"y_pred_proba\": y_pred_proba.tolist(),\n",
        "                    }\n",
        "                )\n",
        "\n",
        "                # Store y_test and y_pred_proba for PR/AUC plotting\n",
        "                model_predictions_data.append({\n",
        "                    \"file name\": file_info.name,\n",
        "                    \"model used\": \"decision_tree\",\n",
        "                    \"scalar_status\": scalar_type,\n",
        "                    \"dt_max_depth\": dt_max_depth,\n",
        "                    \"dt_min_samples_split\": dt_min_samples_split,\n",
        "                    \"y_test\": y_test.tolist(),\n",
        "                    \"y_pred_proba\": y_pred_proba.tolist()\n",
        "                })\n",
        "\n",
        "    return dec_tree_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "swZl1xN2QiHS"
      },
      "outputs": [],
      "source": [
        "def test_random_forest(data_testing, file_info, model_predictions_data):\n",
        "    \"\"\"\n",
        "    ----- inputs -----\n",
        "    data_testing: dict[str=scalingType][str=y/X train/test label][pd.DataFrame],\n",
        "        dictionary contianing,\n",
        "            * string of scaling type (standard scalar, orig),\n",
        "                * string of what dataset grabbing (X_train, X_test, y_train, y_test),\n",
        "                    * corresponding data in a pandas dataframe,\n",
        "    model_predictions_data: list\n",
        "        A list to store dictionaries of y_true and y_pred_proba for PR/AUC curves.\n",
        "    ----- outputs -----\n",
        "    \"\"\"\n",
        "    # results table\n",
        "    rf_results = []\n",
        "\n",
        "    # editing hyperparameters for random forest variables\n",
        "    rf_n_estimators = [100, 300, 500]\n",
        "    rf_max_depths = [None, 10, 50]\n",
        "    rf_min_samples_leaf = [1, 2, 4]\n",
        "\n",
        "    for scalar_type in tqdm(data_testing.keys(), desc=\"random forest scaled vs original\"):\n",
        "        if scalar_type == 'columns':\n",
        "            continue\n",
        "        X_test = data_testing[scalar_type][\"X_test\"]\n",
        "        X_train = data_testing[scalar_type][\"X_train\"]\n",
        "        y_train = data_testing[scalar_type][\"y_train\"]\n",
        "        y_test = data_testing[scalar_type][\"y_test\"]\n",
        "        feature_names = data_testing[\"columns\"].tolist()\n",
        "\n",
        "        for rf_n_estimator in rf_n_estimators:\n",
        "            for rf_max_depth in rf_max_depths:\n",
        "                for rf_min_leaf in rf_min_samples_leaf:\n",
        "                        model = RandomForestClassifier(\n",
        "                                n_estimators=rf_n_estimator,\n",
        "                                max_depth=rf_max_depth,\n",
        "                                min_samples_leaf=rf_min_leaf,\n",
        "                                random_state=RANDOM_STATE,\n",
        "                                n_jobs=-1\n",
        "                        )\n",
        "                        model.fit(X_train, y_train)\n",
        "                        y_pred = model.predict(X_test)\n",
        "                        y_pred_proba = model.predict_proba(X_test)[:, 1] # Probability of the positive class\n",
        "\n",
        "                        # validation\n",
        "                        accuracy = accuracy_score(y_test, y_pred)\n",
        "                        precision = precision_score(y_test, y_pred, zero_division=0)\n",
        "                        recall = recall_score(y_test, y_pred, zero_division=0)\n",
        "                        f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "                        conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "                        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "                        pr_auc = average_precision_score(y_test, y_pred_proba)\n",
        "\n",
        "                        # getting feature importance\n",
        "                        rf_imp = model.feature_importances_\n",
        "                        feature_imp = dict(zip(feature_names, rf_imp))\n",
        "                        feature_imp_json = json.dumps({k: float(v) for k, v in feature_imp.items()})\n",
        "\n",
        "                        # getting permutation importance\n",
        "                        perm = permutation_importance(\n",
        "                          model, X_test, y_test,\n",
        "                          n_repeats=3,\n",
        "                          random_state=RANDOM_STATE,\n",
        "                          scoring=\"f1\"  # or \"accuracy\"\n",
        "                        )\n",
        "\n",
        "                        perm_imp = dict(zip(feature_names, perm.importances_mean))\n",
        "                        perm_imp_json = json.dumps({k: float(v) for k, v in perm_imp.items()})\n",
        "\n",
        "                        # addinghyperparameters to each of these results/outputs\n",
        "                        rf_results.append(\n",
        "                            {\n",
        "                                \"file name\": file_info.name,\n",
        "                                \"accuracy\": accuracy,\n",
        "                                \"precision\": precision,\n",
        "                                \"recall\": recall,\n",
        "                                \"f1\": f1,\n",
        "                                \"roc_auc\": roc_auc,\n",
        "                                \"pr_auc\": pr_auc,\n",
        "                                \"confusion matrix\": conf_matrix,\n",
        "                                \"test size\": TEST_SIZE,\n",
        "                                \"random state\": RANDOM_STATE,\n",
        "                                \"scalar_status\": scalar_type,\n",
        "                                \"y variable used\": Y_COL,\n",
        "                                \"model used\": \"random_forest\",\n",
        "                                \"logistic_reg_c\": np.nan,\n",
        "                                \"lr_ratios\": np.nan,\n",
        "                                \"nn_layers\": np.nan,\n",
        "                                \"nn_neurons\": np.nan,\n",
        "                                \"nn_batch_size\": np.nan,\n",
        "                                \"nn_epochs\": np.nan,\n",
        "                                \"dt_max_depth\": np.nan,\n",
        "                                \"dt_min_samples_split\": np.nan,\n",
        "                                \"svm_c_val\": np.nan,\n",
        "                                \"svm_kernel\": np.nan,\n",
        "                                \"knn_n_neighbor\": np.nan,\n",
        "                                \"knn_weights\": np.nan,\n",
        "                                \"gbm_learning_rate\": np.nan,\n",
        "                                \"gbm_n_estimator\": np.nan,\n",
        "                                \"rf_n_estimators\": rf_n_estimator,\n",
        "                                \"rf_max_depth\": rf_max_depth,\n",
        "                                \"rf_min_samples_leaf\": rf_min_leaf,\n",
        "                                \"coefficient_importance\": feature_imp_json,\n",
        "                                \"permutation_importance\": perm_imp_json,\n",
        "                                \"y_test\": y_test.tolist(), # Added y_test to results\n",
        "                                \"y_pred_proba\": y_pred_proba.tolist(),\n",
        "                            }\n",
        "                        )\n",
        "\n",
        "                        # Store y_test and y_pred_proba for PR/AUC plotting\n",
        "                        model_predictions_data.append({\n",
        "                            \"file name\": file_info.name,\n",
        "                            \"model used\": \"random_forest\",\n",
        "                            \"scalar_status\": scalar_type,\n",
        "                            \"rf_n_estimators\": rf_n_estimator,\n",
        "                            \"rf_max_depth\": rf_max_depth,\n",
        "                            \"rf_min_samples_leaf\": rf_min_leaf,\n",
        "                            \"y_test\": y_test.tolist(),\n",
        "                            \"y_pred_proba\": y_pred_proba.tolist()\n",
        "                        })\n",
        "\n",
        "    return rf_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjtgjul0cTBP"
      },
      "source": [
        "## Preparing models to run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "tudxNRjiSMrV"
      },
      "outputs": [],
      "source": [
        "def run_models_for_file(file_info, model_predictions_data) -> list:\n",
        "    \"\"\"\n",
        "    Goal: return model results for file\n",
        "\n",
        "    Paramaters:\n",
        "        file: str\n",
        "            the name of the file want to model\n",
        "        file_info: Path object\n",
        "            parsed information about the file\n",
        "        model_predictions_data: list\n",
        "            A list to store dictionaries of y_true and y_pred_proba for PR/AUC curves.\n",
        "\n",
        "    Outputs:\n",
        "        all_rows: list\n",
        "            list of the dictionary model results\n",
        "    \"\"\"\n",
        "\n",
        "    df = data_prep(file_info)\n",
        "    if df.empty:\n",
        "        return []\n",
        "\n",
        "    data_testing = get_train_test(df, y_col=Y_COL, scaling_used=True)\n",
        "\n",
        "    model_fns = [\n",
        "          test_logistic_reg,\n",
        "          test_neural_net,\n",
        "          test_knn,\n",
        "          test_decision_tree,\n",
        "          test_random_forest,\n",
        "          test_svm,\n",
        "          test_gbm,\n",
        "    ]\n",
        "\n",
        "    # running each model in the model funcs list to return the results\n",
        "    all_rows = []\n",
        "    for fn in model_fns:\n",
        "        rows = fn(data_testing, file_info, model_predictions_data) # running each function\n",
        "        if rows:\n",
        "            all_rows.extend(rows)\n",
        "\n",
        "    return all_rows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRHGz4nqTzmI"
      },
      "source": [
        "## Running all Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOaqObWPShdS",
        "outputId": "8c0c2351-dce4-4b54-bfc2-4179e1e23e03"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rlogistic regression scaled vs original:   0%|          | 0/3 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "logistic regression scaled vs original:  67%|   | 2/3 [00:25<00:12, 12.71s/it]/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "logistic regression scaled vs original: 100%|| 3/3 [00:45<00:00, 15.10s/it]\n",
            "neural net scaled vs original:   0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x783d144dd8a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\r\u001b[1m1/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x783d144dd8a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rneural net scaled vs original:  67%|   | 2/3 [09:46<04:53, 293.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step \n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "neural net scaled vs original: 100%|| 3/3 [19:57<00:00, 399.08s/it]\n",
            "knn scaled vs original: 100%|| 3/3 [00:13<00:00,  4.48s/it]\n",
            "decision tree scaled vs original: 100%|| 3/3 [00:59<00:00, 19.91s/it]\n",
            "random forest scaled vs original: 100%|| 3/3 [09:37<00:00, 192.51s/it]\n",
            "svm scaled vs original:   0%|          | 0/3 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=20000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=20000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  warnings.warn(\n",
            "svm scaled vs original:  67%|   | 2/3 [00:03<00:01,  1.65s/it]/usr/local/lib/python3.12/dist-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=20000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=20000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  warnings.warn(\n",
            "svm scaled vs original: 100%|| 3/3 [00:06<00:00,  2.25s/it]\n",
            "gbm scaled vs original: 100%|| 3/3 [02:55<00:00, 58.47s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "COMPLETED: k\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# initializing a list to store y_test and y_pred_proba for PR/AUC plotting\n",
        "model_predictions_data = []\n",
        "\n",
        "# running each model to get results\n",
        "rows_results = run_models_for_file(file_info, model_predictions_data) # calling function to run models\n",
        "dataframe_rows_results = pd.DataFrame(rows_results) # converting into a dataframe, so that we can save it\n",
        "dataframe_rows_results.to_csv(f'results_for_{file_info.name}') # saving it into the files section as a CSV\n",
        "print(\"\\n\\nCOMPLETED: k\\n\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
