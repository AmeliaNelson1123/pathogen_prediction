{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3udTPbsQNwnk"
      },
      "source": [
        "# Intro\n",
        "\n",
        "Here is a Google Colab Notebook for model testing. Please input the file name you would like to test, and the predictor column. (The file is set for Listeria and Salmonella/Campylobacter)\n",
        "\n",
        "This is the most up-to-date, tried and tested method I used to develop the baseline code for the model testing AND results.\n",
        "\n",
        "In this notebook, the exact same models that are provided on the competition, official git repo are used. The code is adapted to perform a grid search (search through a list of hyper parameters) to find the best model results.\n",
        "\n",
        "I designed it to save all model results into the local files because of the testing we needed to do for my personal project, however, this code can be streamlined by running a Random Search or a pre-packaged, optimized Grid Search instead of the for loops.\n",
        "\n",
        "\n",
        "NOTE: Please remember to download/save the csv from the Files section on google collab if you want to keep your results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BNvJnHJUEz7"
      },
      "source": [
        "# How to Run the Models\n",
        "\n",
        "## STEP 1: Uploading files to run\n",
        "- Select the files icon and upload the desired file (i.e. SalCampChicken_clean.csv)\n",
        "\n",
        "## STEP 2: Altering file name, y column/predictor, and other main decisions effecting all models\n",
        "- Edit the **global variables**.\n",
        "- NOTE: if you search for STEP 2, the cell(s) where STEP 2 are listed are the cells where you can edit anything!\n",
        "\n",
        "    \n",
        "\n",
        "## Optional STEP 3: Editing or adding a hyperparameter(s)\n",
        "- in each model option (function = *test_\"model name\"*, i.e. test_knn), edit the hyperparameters to search through\n",
        "- NOTE: if you search for STEP 3, the cell(s) where STEP 3 are listed are the cells where you can edit anything!\n",
        "- NOTE: if adding another hyperparameter:\n",
        "  1) Add the hyperparameter as a list\n",
        "  2) Add a for loop\n",
        "  3) Adjust the results_column from STEP 2\n",
        "  4) Adjust each *test_\"Model\"*'s results/output to have the hyperparameters, and put np.nan as the output\n",
        "\n",
        "\n",
        "## Optional STEP 4: Adding a model\n",
        "1) add the model package to the imports section\n",
        "2) add a new codeing section\n",
        "3) name the function *test_\"model\"* for consistency\n",
        "4) copy and paste a similar model\n",
        "5) adjust the hyperparameters, and add or subtract for loops as needed\n",
        "6) edit the hyperparameter options in the results_column (from STEP\\_2) and each of the returning results/outputs in each *test_\"model\"* function\n",
        "\n",
        "## STEP 5: Choosing models to run\n",
        "- in the *run_models_for_file* function, comment or uncomment the lines in the variable **model_fns** to decide which models to run\n",
        "\n",
        "# STEP 6: Running all cells\n",
        "- hit the Run all button as listed above\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RWKExXYn1xd"
      },
      "source": [
        "# How to Run the Results (Scroll down to \"Analyze Results\" section and start there)\n",
        "\n",
        "## If already have results file and do not want to run the modeling section\n",
        "- go to the files icon in google colab, and upload the results file\n",
        "\n",
        "### STEP 1: Change global variables\n",
        "- edit any of the variables, such as the \"primary metric\" to be on your visuals\n",
        "\n",
        "### STEP 2: Runninng analyze\n",
        "- go through and run cells as needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Rq9x-vqmNpj4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    confusion_matrix,\n",
        "    roc_auc_score,\n",
        "    average_precision_score,\n",
        "    PrecisionRecallDisplay\n",
        ")\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Input\n",
        "from keras import backend as K\n",
        "import re\n",
        "from pathlib import Path, PurePosixPath\n",
        "from dataclasses import dataclass, asdict\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "import os\n",
        "import json\n",
        "from sklearn.inspection import permutation_importance\n",
        "import tempfile\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF2zElCsO7oV"
      },
      "source": [
        "# This is where modeling begins.\n",
        "\n",
        "Below, feel free to change test size, column results to save, and the y-col to use.\n",
        "\n",
        "For salmonella/campylobacter, the possible predictor column of your choosing.\n",
        "\n",
        "The Salmonella/campylobacter column was created by an OR statement of the Salmonella OR Campylobacter status."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uM8-xYH7Oy3e"
      },
      "outputs": [],
      "source": [
        "# STEP 2: Edit any of these variables in these sections\n",
        "\n",
        "# saving global variables\n",
        "TEST_SIZE = .22  # for validation (Pick a number between .1-.99 to reflect percentage)\n",
        "RANDOM_STATE = 42  # for repeatability\n",
        "# developing results table to plot\n",
        "\n",
        "DATA_PATH = None\n",
        "# !!!!!! IF YOU ARE RUNNING IN VISUAL STUDIO, UNCOMMENT THE CODE BELOW\n",
        "# ROOT = Path.cwd()\n",
        "# if ROOT.name == \"preparation\":\n",
        "#     ROOT = ROOT.parent\n",
        "# DATA = ROOT / \"data\"\n",
        "\n",
        "\n",
        "# getting in file path\n",
        "try:\n",
        "    file_info = Path(DATA_PATH / \"ListeriaSoil_clean_log.csv\")\n",
        "except:\n",
        "    try:\n",
        "        file_info = Path(\"ListeriaSoil_clean_log.csv\")\n",
        "    except Exception as e:\n",
        "        raise e\n",
        "\n",
        "Y_COL = \"binary_listeria_presense\"\n",
        "\n",
        "# If want all strings/catagorical data to be encoded in 1-hot vectors\n",
        "# (aka want to transform arbitrary strings into integer values)\n",
        "ENCODE_STR = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQryAxklQMa7"
      },
      "source": [
        "# Preparing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "T5-KpYLtP6hZ"
      },
      "outputs": [],
      "source": [
        "def data_prep(file_info):\n",
        "    \"\"\"\n",
        "    ----- inputs -----\n",
        "    file_info: Path object\n",
        "        file wanting to process\n",
        "    ----- outputs ----\n",
        "    df: pandas df\n",
        "        processed anonymzied data (string columns representing intervals split into min and max, then put as minimum and maximum values for those columns)\n",
        "    \"\"\"\n",
        "\n",
        "    df = pd.read_csv(Path(file_info.name))\n",
        "\n",
        "    # Drop 'index' column if it exists, as it's typically an artifact and not a feature\n",
        "    if 'index' in df.columns:\n",
        "        df = df.drop(columns=['index'])\n",
        "\n",
        "    # if salcamp column, then adding it to the dataset\n",
        "    if Y_COL == \"Salmon_or_camp_test\":\n",
        "        df[\"Salmon_or_camp_test\"] = (df[\"CampylobacterAnalysis30ml\"] == \"Positive\") | (df[\"SalmonellaSPAnalysis\"] == \"Positive\")\n",
        "    if Y_COL == \"binary_listeria_presense\":\n",
        "        original_listeria_col = 'Number of Listeria isolates obtained'\n",
        "        df['binary_listeria_presense'] = [row_val if row_val == 0 else 1 for row_val in df[original_listeria_col]]\n",
        "        # Drop the original column to prevent data leakage\n",
        "        if original_listeria_col in df.columns:\n",
        "            df = df.drop(columns=[original_listeria_col])\n",
        "\n",
        "    # switching missing values and weird failures in writing to np.inf bc pandas didnt handle properly\n",
        "    df = df.replace(\"#NAME?\", -np.inf)\n",
        "    df = df.fillna(-np.inf)\n",
        "\n",
        "    # replacing inf with max number that is not max number + 100 in dict (FOR NOT JUST 99999999)\n",
        "    df = df.replace(np.inf, 99999)\n",
        "    # replacing -inf with min number (not -inf) - 100 in dict (FOR NOT JUST -99999999)\n",
        "    df = df.replace(-np.inf, -99999)\n",
        "\n",
        "    # Drop unwanted cluster columns if they exist\n",
        "    cols_to_drop = ['scaled_cluster_kmeans', 'cluster_kmeans']\n",
        "    df = df.drop(columns=[c for c in cols_to_drop if c in df.columns])\n",
        "\n",
        "    df = df.dropna(axis=1, how=\"all\")\n",
        "\n",
        "    if ENCODE_STR:\n",
        "        df = pd.get_dummies(df)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQZ0shc5QI89"
      },
      "source": [
        "# Splitting into Train and Test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QT3EkeBZQGOq"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_train_test(\n",
        "    df, y_col=Y_COL, scaling_used=True\n",
        "):\n",
        "    \"\"\"\n",
        "    ----- inputs -----\n",
        "    df: pandas dict\n",
        "        processed data (all numerics)\n",
        "    y_col: str\n",
        "        string of y labels\n",
        "    test_size: int\n",
        "        % want test set to be of full data\n",
        "    scaling_used: boolean\n",
        "        whether to test scaled data and original data (True) or only original data (False)\n",
        "    ----- outputs ----\n",
        "    data_testing: dict[str=scalingType][str=y/X train/test label][pd.DataFrame]\n",
        "        dictionary contianing\n",
        "            * string of scaling type (standard scalar, orig)\n",
        "                * string of what dataset grabbing (X_train, X_test, y_train, y_test)\n",
        "                    * corresponding data in a pandas dataframe\n",
        "        \"\n",
        "    \"\"\"\n",
        "\n",
        "    # indexes for test set\n",
        "    X = df.drop(columns=Y_COL)\n",
        "    y = df[Y_COL]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
        "\n",
        "    # data columns\n",
        "    data_columns = X.columns\n",
        "\n",
        "    if scaling_used:  # if want to run on scaled and original data\n",
        "        # testing all with and without scaled data\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train.values)\n",
        "        X_test_scaled = scaler.transform(X_test.values)\n",
        "\n",
        "        data_testing = {\n",
        "            \"columns\": data_columns,\n",
        "            \"standard_scalar\": {\n",
        "                \"X_train\": X_train_scaled,\n",
        "                \"X_test\": X_test_scaled,\n",
        "                \"y_train\": y_train,  # using unscaled y\n",
        "                \"y_test\": y_test,  # using unscaled y\n",
        "            },\n",
        "            \"orig\": {\n",
        "                \"X_train\": X_train,\n",
        "                \"X_test\": X_test,\n",
        "                \"y_train\": y_train,\n",
        "                \"y_test\": y_test,\n",
        "            },\n",
        "        }\n",
        "\n",
        "        return data_testing\n",
        "\n",
        "    else:  # if only want to run on original data\n",
        "        data_testing = {\n",
        "            \"columns\": data_columns,\n",
        "            \"orig\": {\n",
        "                \"X_train\": X_train,\n",
        "                \"X_test\": X_test,\n",
        "                \"y_train\": y_train,\n",
        "                \"y_test\": y_test,\n",
        "            }\n",
        "        }\n",
        "        return data_testing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atQqkgpGQjFV"
      },
      "source": [
        "# Modeling Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fLc6J0NFRamd"
      },
      "outputs": [],
      "source": [
        "def test_svm(data_testing, file_info, model_predictions_data):\n",
        "    \"\"\"\n",
        "    ----- inputs -----\n",
        "    data_testing: dict[str=scalingType][str=y/X train/test label][pd.DataFrame]\n",
        "        dictionary contianing\n",
        "            * string of scaling type (standard scalar, orig)\n",
        "                * string of what dataset grabbing (X_train, X_test, y_train, y_test)\n",
        "                    * corresponding data in a pandas dataframe\n",
        "    model_predictions_data: list\n",
        "        A list to store dictionaries of y_true and y_pred_proba for PR/AUC curves.\n",
        "    ----- outputs ----\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # results table\n",
        "    svm_results = []\n",
        "\n",
        "    # STEP 3: can edit HYPERPARAMETERs: for svm variables\n",
        "    c_vals = [1, 4]\n",
        "    svm_kernels = ['linear', 'rbf']\n",
        "\n",
        "    # grid searching model results for svm on all types of data with all types of inputs\n",
        "    for scalar_type in tqdm(data_testing.keys(), desc=\"svm scaled vs original\"):\n",
        "        if scalar_type == 'columns':\n",
        "            continue\n",
        "        X_test = data_testing[scalar_type][\"X_test\"]\n",
        "        X_train = data_testing[scalar_type][\"X_train\"]\n",
        "        y_train = data_testing[scalar_type][\"y_train\"]\n",
        "        y_test = data_testing[scalar_type][\"y_test\"]\n",
        "        feature_names = data_testing[\"columns\"].tolist()\n",
        "\n",
        "        # going through possible svm combos\n",
        "        for c_val in c_vals:\n",
        "            for svm_kernel in svm_kernels:\n",
        "                # modeling portion\n",
        "                # SVC needs probability=True to use predict_proba, which can be slower\n",
        "                model = SVC(C=c_val, kernel=svm_kernel, max_iter=20000, probability=True)\n",
        "                model.fit(X_train, y_train)\n",
        "                y_pred = model.predict(X_test)\n",
        "                y_pred_proba = model.predict_proba(X_test)[:, 1] # Probability of the positive class\n",
        "\n",
        "                # validation\n",
        "                accuracy = accuracy_score(y_test, y_pred)\n",
        "                precision = precision_score(y_test, y_pred, zero_division=0)\n",
        "                recall = recall_score(y_test, y_pred, zero_division=0)\n",
        "                f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "                conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "                roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "                pr_auc = average_precision_score(y_test, y_pred_proba)\n",
        "\n",
        "                # getting feature importance\n",
        "                coefficients = None\n",
        "                feature_imp = None\n",
        "                feature_imp_json = None\n",
        "                if svm_kernel == 'linear':\n",
        "                    coefficients = model.coef_.ravel()\n",
        "                    feature_imp = dict(zip(feature_names, coefficients))\n",
        "                    feature_imp_json = json.dumps({k: float(v) for k, v in feature_imp.items()})\n",
        "\n",
        "                # getting permutation importance\n",
        "                perm = permutation_importance(\n",
        "                    model, X_test, y_test,\n",
        "                    n_repeats=3,\n",
        "                    random_state=RANDOM_STATE,\n",
        "                    scoring=\"f1\"  # or \"accuracy\"\n",
        "                )\n",
        "\n",
        "                perm_imp = dict(zip(feature_names, perm.importances_mean))\n",
        "                perm_imp_json = json.dumps({k: float(v) for k, v in perm_imp.items()})\n",
        "\n",
        "                # STEP 3, add any hyperparameters to each of these results/outputs: saving results to dict\n",
        "                svm_results.append(\n",
        "                    {\n",
        "                        \"file name\": file_info.name,\n",
        "                        \"accuracy\": accuracy,\n",
        "                        \"precision\": precision,\n",
        "                        \"recall\": recall,\n",
        "                        \"f1\": f1,\n",
        "                        \"roc_auc\": roc_auc,\n",
        "                        \"pr_auc\": pr_auc,\n",
        "                        \"confusion matrix\": conf_matrix,\n",
        "                        \"test size\": TEST_SIZE,\n",
        "                        \"random state\": RANDOM_STATE,\n",
        "                        \"scalar_status\": scalar_type,\n",
        "                        \"y variable used\": Y_COL,\n",
        "                        \"model used\": \"svm\",\n",
        "                        \"logistic_reg_c\": None,\n",
        "                        \"lr_ratios\": None,\n",
        "                        \"nn_layers\": None,\n",
        "                        \"nn_neurons\": None,\n",
        "                        \"nn_batch_size\": None,\n",
        "                        \"nn_epochs\": None,\n",
        "                        \"dt_max_depth\": None,\n",
        "                        \"dt_min_samples_split\": None,\n",
        "                        \"svm_c_val\": c_val,\n",
        "                        \"svm_kernel\": svm_kernel,\n",
        "                        \"knn_weights\": None,\n",
        "                        \"gbm_learning_rate\": None,\n",
        "                        \"gbm_n_estimator\": None,\n",
        "                        \"rf_n_estimators\": None,\n",
        "                        \"rf_max_depth\": None,\n",
        "                        \"rf_min_samples_leaf\": None,\n",
        "                        \"coefficient_importance\": feature_imp_json,\n",
        "                        \"permutation_importance\": perm_imp_json,\n",
        "                        \"y_pred_proba\": y_pred_proba.tolist(),\n",
        "                    }\n",
        "                )\n",
        "\n",
        "                # Store y_test and y_pred_proba for PR/AUC plotting\n",
        "                model_predictions_data.append({\n",
        "                    \"file name\": file_info.name,\n",
        "                    \"model used\": \"svm\",\n",
        "                    \"scalar_status\": scalar_type,\n",
        "                    \"svm_c_val\": c_val,\n",
        "                    \"svm_kernel\": svm_kernel,\n",
        "                    \"y_test\": y_test.tolist(),\n",
        "                    \"y_pred_proba\": y_pred_proba.tolist()\n",
        "                })\n",
        "\n",
        "    return svm_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RveNGUPxQnqc"
      },
      "outputs": [],
      "source": [
        "def test_logistic_reg(data_testing, file_info, model_predictions_data):\n",
        "    \"\"\"\n",
        "    ----- inputs -----\n",
        "    data_testing: dict[str=scalingType][str=y/X train/test label][pd.DataFrame]\n",
        "        dictionary contianing\n",
        "            * string of scaling type (standard scalar, orig)\n",
        "                * string of what dataset grabbing (X_train, X_test, y_train, y_test)\n",
        "                    * corresponding data in a pandas dataframe\n",
        "    model_predictions_data: list\n",
        "        A list to store dictionaries of y_true and y_pred_proba for PR/AUC curves.\n",
        "    ----- outputs ----\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    log_reg_results = []\n",
        "\n",
        "    # STEP 3: can edit HYPERPARAMETERS: for logistic regression variables\n",
        "    # c_vals = [0.0001, 0.001, 0.01, 0.1, 1, 2, 4, 8, 32, 56, 100]\n",
        "    c_vals = [0.01, 0.1, 1, 4, 8]\n",
        "    # lr_ratios = [0, 0.5, 1] # 0 = l2 penalty, 1 = l1 penalty, 0.5 = elasticnet penalty (both L1 and L2)\n",
        "    lr_ratios = [\n",
        "        0,\n",
        "        1,\n",
        "    ]  # 0 = l2 penalty, 1 = l1 penalty, 0.5 = elasticnet penalty (both L1 and L2)\n",
        "\n",
        "    # grid searching model results for log reg on all types of data with all types of inputs\n",
        "    for scalar_type in tqdm(data_testing.keys(), desc=\"logistic regression scaled vs original\"):\n",
        "\n",
        "        # breakpoint()\n",
        "\n",
        "        if scalar_type == 'columns':\n",
        "            continue\n",
        "        # print(\"data testing is :\", type(data_testing), \" \\n\", data_testing)\n",
        "        X_test = data_testing[scalar_type][\"X_test\"]\n",
        "        X_train = data_testing[scalar_type][\"X_train\"]\n",
        "        y_train = data_testing[scalar_type][\"y_train\"]\n",
        "        y_test = data_testing[scalar_type][\"y_test\"]\n",
        "        feature_names = data_testing[\"columns\"].tolist()\n",
        "\n",
        "        # breakpoint()\n",
        "\n",
        "        # going through possible log reg combos\n",
        "        for c_val in c_vals:\n",
        "            for lr_rat in lr_ratios:\n",
        "                # modeling portion\n",
        "                model = LogisticRegression(C=c_val, l1_ratio=lr_rat)\n",
        "                model.fit(X_train, y_train)\n",
        "                y_pred = model.predict(X_test)\n",
        "                y_pred_proba = model.predict_proba(X_test)[:, 1] # Probability of the positive class\n",
        "\n",
        "                # validation\n",
        "                accuracy = accuracy_score(y_test, y_pred)\n",
        "                precision = precision_score(y_test, y_pred, zero_division=0)\n",
        "                recall = recall_score(y_test, y_pred, zero_division=0)\n",
        "                f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "                conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "                roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "                pr_auc = average_precision_score(y_test, y_pred_proba)\n",
        "\n",
        "                # getting feature importance\n",
        "                coefficients = model.coef_.ravel()\n",
        "                feature_imp = dict(zip(feature_names, coefficients))\n",
        "                feature_imp_json = json.dumps({k: float(v) for k, v in feature_imp.items()})\n",
        "\n",
        "                # getting permutation importance\n",
        "                perm = permutation_importance(\n",
        "                    model, X_test, y_test,\n",
        "                    n_repeats=10,\n",
        "                    random_state=RANDOM_STATE,\n",
        "                    scoring=\"f1\"  # or \"accuracy\"\n",
        "                )\n",
        "\n",
        "                perm_imp = dict(zip(feature_names, perm.importances_mean))\n",
        "                perm_imp_json = json.dumps({k: float(v) for k, v in perm_imp.items()})\n",
        "\n",
        "                # STEP 3, add any hyperparameters to each of these results/outputs: saving results to dictfile_path: str\n",
        "                log_reg_results.append(\n",
        "                    {\n",
        "                        \"file name\": file_info.name,\n",
        "                        \"accuracy\": accuracy,\n",
        "                        \"precision\": precision,\n",
        "                        \"recall\": recall,\n",
        "                        \"f1\": f1,\n",
        "                        \"roc_auc\": roc_auc,\n",
        "                        \"pr_auc\": pr_auc,\n",
        "                        \"confusion matrix\": conf_matrix,\n",
        "                        \"test size\": TEST_SIZE,\n",
        "                        \"random state\": RANDOM_STATE,\n",
        "                        \"scalar_status\": scalar_type,\n",
        "                        \"y variable used\": Y_COL,\n",
        "                        \"model used\": \"logistic regression\",\n",
        "                        \"logistic_reg_c\": c_val,\n",
        "                        \"lr_ratios\": lr_rat,\n",
        "                        \"nn_layers\": np.nan,\n",
        "                        \"nn_neurons\": np.nan,\n",
        "                        \"nn_batch_size\": np.nan,\n",
        "                        \"nn_epochs\": np.nan,\n",
        "                        \"dt_max_depth\": np.nan,\n",
        "                        \"dt_min_samples_split\": np.nan,\n",
        "                        \"svm_c_val\": np.nan,\n",
        "                        \"svm_kernel\": np.nan,\n",
        "                        \"knn_n_neighbor\": np.nan,\n",
        "                        \"knn_weights\": np.nan,\n",
        "                        \"gbm_learning_rate\": np.nan,\n",
        "                        \"gbm_n_estimator\": np.nan,\n",
        "                        \"rf_n_estimators\": np.nan,\n",
        "                        \"rf_max_depth\": np.nan,\n",
        "                        \"rf_min_samples_leaf\": np.nan,\n",
        "                        \"coefficient_importance\": feature_imp_json,\n",
        "                        \"permutation_importance\": perm_imp_json,\n",
        "                        \"y_pred_proba\": y_pred_proba.tolist(),\n",
        "                    }\n",
        "                )\n",
        "\n",
        "                # Store y_test and y_pred_proba for PR/AUC plotting\n",
        "                model_predictions_data.append({\n",
        "                    \"file name\": file_info.name,\n",
        "                    \"model used\": \"logistic regression\",\n",
        "                    \"scalar_status\": scalar_type,\n",
        "                    \"logistic_reg_c\": c_val,\n",
        "                    \"lr_ratios\": lr_rat,\n",
        "                    \"y_test\": y_test.tolist(),\n",
        "                    \"y_pred_proba\": y_pred_proba.tolist()\n",
        "                })\n",
        "\n",
        "    return log_reg_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "T7iYfaUXRhX-"
      },
      "outputs": [],
      "source": [
        "def test_knn(data_testing, file_info, model_predictions_data):\n",
        "    \"\"\"\n",
        "    ----- inputs -----\n",
        "    data_testing: dict[str=scalingType][str=y/X train/test label][pd.DataFrame]\n",
        "        dictionary contianing\n",
        "            * string of scaling type (standard scalar, orig)\n",
        "                * string of what dataset grabbing (X_train, X_test, y_train, y_test)\n",
        "                    * corresponding data in a pandas dataframe\n",
        "    model_predictions_data: list\n",
        "        A list to store dictionaries of y_true and y_pred_proba for PR/AUC curves.\n",
        "    ----- outputs ----\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # results table\n",
        "    knn_results = []\n",
        "\n",
        "    # STEP 3: can edit HYPERPARAMETERS: for knn variables\n",
        "    knn_n_neighbors = [2, 5, 10, 15, 20]\n",
        "    weights = ['uniform', 'distance']\n",
        "\n",
        "    # grid searching model results for knn on all types of data with all types of inputs\n",
        "    for scalar_type in tqdm(data_testing.keys(), desc=\"knn scaled vs original\"):\n",
        "        if scalar_type == 'columns':\n",
        "            continue\n",
        "        X_test = data_testing[scalar_type][\"X_test\"]\n",
        "        X_train = data_testing[scalar_type][\"X_train\"]\n",
        "        y_train = data_testing[scalar_type][\"y_train\"]\n",
        "        y_test = data_testing[scalar_type][\"y_test\"]\n",
        "        feature_names = data_testing[\"columns\"].tolist()\n",
        "\n",
        "        # going through possible KNN combos\n",
        "        for knn_n_neighbor in knn_n_neighbors:\n",
        "            for weight in weights:\n",
        "                # modeling portion\n",
        "                model = KNeighborsClassifier(\n",
        "                    n_neighbors=knn_n_neighbor, weights=weight\n",
        "                )\n",
        "                model.fit(X_train, y_train)\n",
        "                y_pred = model.predict(X_test)\n",
        "                y_pred_proba = model.predict_proba(X_test)[:, 1] # Probability of the positive class\n",
        "\n",
        "                # validation\n",
        "                accuracy = accuracy_score(y_test, y_pred)\n",
        "                precision = precision_score(y_test, y_pred, zero_division=0)\n",
        "                recall = recall_score(y_test, y_pred, zero_division=0)\n",
        "                f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "                conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "                roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "                pr_auc = average_precision_score(y_test, y_pred_proba)\n",
        "\n",
        "                # getting permutation importance\n",
        "                perm = permutation_importance(\n",
        "                    model, X_test, y_test,\n",
        "                    n_repeats=3, # reduced number to speed things up significantly\n",
        "                    random_state=RANDOM_STATE,\n",
        "                    scoring=\"f1\"  # or \"accuracy\"\n",
        "                )\n",
        "\n",
        "                perm_imp = dict(zip(feature_names, perm.importances_mean))\n",
        "                perm_imp_json = json.dumps({k: float(v) for k, v in perm_imp.items()})\n",
        "\n",
        "                # STEP 3, add any hyperparameters to each of these results/outputs: saving results to dict\n",
        "                knn_results.append(\n",
        "                    {\n",
        "                        \"file name\": file_info.name,\n",
        "                        \"accuracy\": accuracy,\n",
        "                        \"precision\": precision,\n",
        "                        \"recall\": recall,\n",
        "                        \"f1\": f1,\n",
        "                        \"roc_auc\": roc_auc,\n",
        "                        \"pr_auc\": pr_auc,\n",
        "                        \"confusion matrix\": conf_matrix,\n",
        "                        \"test size\": TEST_SIZE,\n",
        "                        \"random state\": RANDOM_STATE,\n",
        "                        \"scalar_status\": scalar_type,\n",
        "                        \"y variable used\": Y_COL,\n",
        "                        \"model used\": \"knn\",\n",
        "                        \"logistic_reg_c\": np.nan,\n",
        "                        \"lr_ratios\": np.nan,\n",
        "                        \"nn_layers\": np.nan,\n",
        "                        \"nn_neurons\": np.nan,\n",
        "                        \"nn_batch_size\": np.nan,\n",
        "                        \"nn_epochs\": np.nan,\n",
        "                        \"dt_max_depth\": np.nan,\n",
        "                        \"dt_min_samples_split\": np.nan,\n",
        "                        \"svm_c_val\": np.nan,\n",
        "                        \"svm_kernel\": np.nan,\n",
        "                        \"knn_n_neighbor\": knn_n_neighbor,\n",
        "                        \"knn_weights\": weight,\n",
        "                        \"gbm_learning_rate\": np.nan,\n",
        "                        \"gbm_n_estimator\": np.nan,\n",
        "                        \"rf_n_estimators\": np.nan,\n",
        "                        \"rf_max_depth\": np.nan,\n",
        "                        \"rf_min_samples_leaf\": np.nan,\n",
        "                        \"coefficient_importance\": np.nan,\n",
        "                        \"permutation_importance\": perm_imp_json,\n",
        "                        \"y_pred_proba\": y_pred_proba.tolist(),\n",
        "                    }\n",
        "                )\n",
        "\n",
        "                # Store y_test and y_pred_proba for PR/AUC plotting\n",
        "                model_predictions_data.append({\n",
        "                    \"file name\": file_info.name,\n",
        "                    \"model used\": \"knn\",\n",
        "                    \"scalar_status\": scalar_type,\n",
        "                    \"knn_n_neighbor\": knn_n_neighbor,\n",
        "                    \"knn_weights\": weight,\n",
        "                    \"y_test\": y_test.tolist(),\n",
        "                    \"y_pred_proba\": y_pred_proba.tolist()\n",
        "                })\n",
        "\n",
        "    return knn_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ckfrxeJEQyb1"
      },
      "outputs": [],
      "source": [
        "def test_neural_net(data_testing, file_info, model_predictions_data):\n",
        "    \"\"\"\n",
        "    ----- inputs -----\n",
        "    data_testing: dict[str=scalingType][str=y/X train/test label][pd.DataFrame]\n",
        "        dictionary contianing\n",
        "            * string of scaling type (standard scalar, orig)\n",
        "                * string of what dataset grabbing (X_train, X_test, y_train, y_test)\n",
        "                    * corresponding data in a pandas dataframe\n",
        "    model_predictions_data: list\n",
        "        A list to store dictionaries of y_true and y_pred_proba for PR/AUC curves.\n",
        "    ----- outputs ----\n",
        "\n",
        "    \"\"\"\n",
        "    # results table\n",
        "    neur_net_results = []\n",
        "\n",
        "    #  STEP 3: can edit HYPERPARAMETERS: for neural net\n",
        "    nn_layers_list = [1, 2, 3, 4]\n",
        "    nn_neurons_list = [16, 32, 64, 128, 256]\n",
        "    nn_batch_size_list = [32, 64, 128, 256]\n",
        "    nn_epochs_list = [5, 10, 20]\n",
        "\n",
        "    # grid searching model results for neural net on all types of data with all types of inputs\n",
        "    for scalar_type in tqdm(data_testing.keys(), desc=\"neural net scaled vs original\"):\n",
        "        if scalar_type == 'columns':\n",
        "            continue\n",
        "        X_test = data_testing[scalar_type][\"X_test\"]\n",
        "        X_train = data_testing[scalar_type][\"X_train\"]\n",
        "        y_train = data_testing[scalar_type][\"y_train\"]\n",
        "        y_test = data_testing[scalar_type][\"y_test\"]\n",
        "\n",
        "        # going through possible neural net combos\n",
        "        for nn_layers in nn_layers_list:\n",
        "            for nn_neurons in nn_neurons_list:\n",
        "                for nn_batch_size in nn_batch_size_list:\n",
        "                    for nn_epochs in nn_epochs_list:\n",
        "                        # modeling portion\n",
        "                        model = Sequential()\n",
        "                        model.add(Input(shape=(X_train.shape[1],)))\n",
        "                        for _ in range(nn_layers):\n",
        "                            model.add(Dense(nn_neurons, activation=\"relu\"))\n",
        "                        model.add(Dense(1, activation=\"sigmoid\"))\n",
        "                        model.compile(\n",
        "                            optimizer=\"adam\",\n",
        "                            loss=\"binary_crossentropy\",\n",
        "                            metrics=[\"accuracy\"],\n",
        "                        )\n",
        "                        model.fit(\n",
        "                            X_train,\n",
        "                            y_train,\n",
        "                            epochs=nn_epochs,\n",
        "                            batch_size=nn_batch_size,\n",
        "                            verbose=0,\n",
        "                        )\n",
        "                        y_pred_proba = model.predict(X_test).flatten() # Probabilities\n",
        "                        y_pred = (y_pred_proba > 0.5).astype(int) # Predicted classes\n",
        "\n",
        "                        # validation\n",
        "                        accuracy = accuracy_score(y_test, y_pred)\n",
        "                        precision = precision_score(y_test, y_pred, zero_division=0)\n",
        "                        recall = recall_score(y_test, y_pred, zero_division=0)\n",
        "                        f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "                        conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "                        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "                        pr_auc = average_precision_score(y_test, y_pred_proba)\n",
        "\n",
        "                        # STEP 3, add any hyperparameters to each of these results/outputs: saving results to dict\n",
        "                        neur_net_results.append(\n",
        "                            {\n",
        "                                \"file name\": file_info.name,\n",
        "                                \"accuracy\": accuracy,\n",
        "                                \"precision\": precision,\n",
        "                                \"recall\": recall,\n",
        "                                \"f1\": f1,\n",
        "                                \"roc_auc\": roc_auc,\n",
        "                                \"pr_auc\": pr_auc,\n",
        "                                \"confusion matrix\": conf_matrix,\n",
        "                                \"test size\": TEST_SIZE,\n",
        "                                \"random state\": RANDOM_STATE,\n",
        "                                \"scalar_status\": scalar_type,\n",
        "                                \"y variable used\": Y_COL,\n",
        "                                \"model used\": \"neural net\",\n",
        "                                \"logistic_reg_c\": np.nan,\n",
        "                                \"lr_ratios\": np.nan,\n",
        "                                \"nn_layers\": nn_layers,\n",
        "                                \"nn_neurons\": nn_neurons,\n",
        "                                \"nn_batch_size\": nn_batch_size,\n",
        "                                \"nn_epochs\": nn_epochs,\n",
        "                                \"dt_max_depth\": np.nan,\n",
        "                                \"dt_min_samples_split\": np.nan,\n",
        "                                \"svm_c_val\": np.nan,\n",
        "                                \"svm_kernel\": np.nan,\n",
        "                                \"knn_n_neighbor\": np.nan,\n",
        "                                \"knn_weights\": np.nan,\n",
        "                                \"gbm_learning_rate\": np.nan,\n",
        "                                \"gbm_n_estimator\": np.nan,\n",
        "                                \"rf_n_estimators\": np.nan,\n",
        "                                \"rf_max_depth\": np.nan,\n",
        "                                \"rf_min_samples_leaf\": np.nan,\n",
        "                                \"coefficient_importance\": np.nan,\n",
        "                                \"permutation_importance\": np.nan,\n",
        "                                \"y_pred_proba\": y_pred_proba.tolist(),\n",
        "                            }\n",
        "                        )\n",
        "\n",
        "                        # Store y_test and y_pred_proba for PR/AUC plotting\n",
        "                        model_predictions_data.append({\n",
        "                            \"file name\": file_info.name,\n",
        "                            \"model used\": \"neural net\",\n",
        "                            \"scalar_status\": scalar_type,\n",
        "                            \"nn_layers\": nn_layers,\n",
        "                            \"nn_neurons\": nn_neurons,\n",
        "                            \"nn_batch_size\": nn_batch_size,\n",
        "                            \"nn_epochs\": nn_epochs,\n",
        "                            \"y_test\": y_test.tolist(),\n",
        "                            \"y_pred_proba\": y_pred_proba.tolist()\n",
        "                        })\n",
        "\n",
        "    return neur_net_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DmdtiU0wSBSY"
      },
      "outputs": [],
      "source": [
        "def test_gbm(data_testing, file_info, model_predictions_data):\n",
        "    \"\"\"\n",
        "    ----- inputs -----\n",
        "    data_testing: dict[str=scalingType][str=y/X train/test label][pd.DataFrame]\n",
        "        dictionary contianing\n",
        "            * string of scaling type (standard scalar, orig)\n",
        "                * string of what dataset grabbing (X_train, X_test, y_train, y_test)\n",
        "                    * corresponding data in a pandas dataframe\n",
        "    model_predictions_data: list\n",
        "        A list to store dictionaries of y_true and y_pred_proba for PR/AUC curves.\n",
        "    ----- outputs ----\n",
        "\n",
        "    \"\"\"\n",
        "    # results table\n",
        "    gbm_results = []\n",
        "\n",
        "    #  STEP 3: can edit HYPERPARAMETERS: for gbm variables\n",
        "    gbm_learning_rates = [0.01, 0.05, 0.1, 0.2]\n",
        "    gbm_n_estimators = [100, 200, 400, 800]\n",
        "\n",
        "    # grid searching model results for gbm on all types of data with all types of inputs\n",
        "    for scalar_type in tqdm(data_testing.keys(), desc=\"gbm scaled vs original\"):\n",
        "        if scalar_type == 'columns':\n",
        "            continue\n",
        "        X_test = data_testing[scalar_type][\"X_test\"]\n",
        "        X_train = data_testing[scalar_type][\"X_train\"]\n",
        "        y_train = data_testing[scalar_type][\"y_train\"]\n",
        "        y_test = data_testing[scalar_type][\"y_test\"]\n",
        "        feature_names = data_testing[\"columns\"].tolist()\n",
        "\n",
        "        # going through possible gbm combos\n",
        "        for gbm_learning_rate in gbm_learning_rates:\n",
        "            for gbm_n_estimator in gbm_n_estimators:\n",
        "                # modeling portion\n",
        "                model = GradientBoostingClassifier(\n",
        "                    learning_rate=gbm_learning_rate, n_estimators=gbm_n_estimator\n",
        "                )\n",
        "                model.fit(X_train, y_train)\n",
        "                y_pred = model.predict(X_test)\n",
        "                y_pred_proba = model.predict_proba(X_test)[:, 1] # Probability of the positive class\n",
        "\n",
        "                # validation\n",
        "                accuracy = accuracy_score(y_test, y_pred)\n",
        "                precision = precision_score(y_test, y_pred, zero_division=0)\n",
        "                recall = recall_score(y_test, y_pred, zero_division=0)\n",
        "                f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "                conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "                roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "                pr_auc = average_precision_score(y_test, y_pred_proba)\n",
        "\n",
        "                # getting feature importance\n",
        "                gbm_imp = model.feature_importances_\n",
        "                feature_imp = dict(zip(feature_names, gbm_imp))\n",
        "                feature_imp_json = json.dumps({k: float(v) for k, v in feature_imp.items()})\n",
        "\n",
        "                # getting permutation importance\n",
        "                perm = permutation_importance(\n",
        "                    model, X_test, y_test,\n",
        "                    n_repeats=10,\n",
        "                    random_state=RANDOM_STATE,\n",
        "                    scoring=\"f1\"  # or \"accuracy\"\n",
        "                )\n",
        "\n",
        "                perm_imp = dict(zip(feature_names, perm.importances_mean))\n",
        "                perm_imp_json = json.dumps({k: float(v) for k, v in perm_imp.items()})\n",
        "\n",
        "                # STEP 3, add any hyperparameters to each of these results/outputs: saving results to dict\n",
        "                gbm_results.append(\n",
        "                    {\n",
        "                        \"file name\": file_info.name,\n",
        "                        \"accuracy\": accuracy,\n",
        "                        \"precision\": precision,\n",
        "                        \"recall\": recall,\n",
        "                        \"f1\": f1,\n",
        "                        \"roc_auc\": roc_auc,\n",
        "                        \"pr_auc\": pr_auc,\n",
        "                        \"confusion matrix\": conf_matrix,\n",
        "                        \"test size\": TEST_SIZE,\n",
        "                        \"random state\": RANDOM_STATE,\n",
        "                        \"scalar_status\": scalar_type,\n",
        "                        \"y variable used\": Y_COL,\n",
        "                        \"model used\": \"gbm\",\n",
        "                        \"logistic_reg_c\": np.nan,\n",
        "                        \"lr_ratios\": np.nan,\n",
        "                        \"nn_layers\": np.nan,\n",
        "                        \"nn_neurons\": np.nan,\n",
        "                        \"nn_batch_size\": np.nan,\n",
        "                        \"nn_epochs\": np.nan,\n",
        "                        \"dt_max_depth\": np.nan,\n",
        "                        \"dt_min_samples_split\": np.nan,\n",
        "                        \"svm_c_val\": np.nan,\n",
        "                        \"svm_kernel\": np.nan,\n",
        "                        \"knn_n_neighbor\": np.nan,\n",
        "                        \"knn_weights\": np.nan,\n",
        "                        \"gbm_learning_rate\": gbm_learning_rate,\n",
        "                        \"gbm_n_estimator\": gbm_n_estimator,\n",
        "                        \"rf_n_estimators\": np.nan,\n",
        "                        \"rf_max_depth\": np.nan,\n",
        "                        \"rf_min_samples_leaf\": np.nan,\n",
        "                        \"coefficient_importance\": feature_imp_json,\n",
        "                        \"permutation_importance\": perm_imp_json,\n",
        "                        \"y_pred_proba\": y_pred_proba.tolist(),\n",
        "                    }\n",
        "                )\n",
        "\n",
        "                # Store y_test and y_pred_proba for PR/AUC plotting\n",
        "                model_predictions_data.append({\n",
        "                    \"file name\": file_info.name,\n",
        "                    \"model used\": \"gbm\",\n",
        "                    \"scalar_status\": scalar_type,\n",
        "                    \"gbm_learning_rate\": gbm_learning_rate,\n",
        "                    \"gbm_n_estimator\": gbm_n_estimator,\n",
        "                    \"y_test\": y_test.tolist(),\n",
        "                    \"y_pred_proba\": y_pred_proba.tolist()\n",
        "                })\n",
        "    return gbm_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "zEMr1gIvQ8SJ"
      },
      "outputs": [],
      "source": [
        "def test_decision_tree(data_testing, file_info, model_predictions_data):\n",
        "    \"\"\"\n",
        "    ----- inputs -----\n",
        "    data_testing: dict[str=scalingType][str=y/X train/test label][pd.DataFrame]\n",
        "        dictionary contianing\n",
        "            * string of scaling type (standard scalar, orig)\n",
        "                * string of what dataset grabbing (X_train, X_test, y_train, y_test)\n",
        "                    * corresponding data in a pandas dataframe\n",
        "    model_predictions_data: list\n",
        "        A list to store dictionaries of y_true and y_pred_proba for PR/AUC curves.\n",
        "    ----- outputs ----\n",
        "\n",
        "    \"\"\"\n",
        "    # results table\n",
        "    dec_tree_results = []\n",
        "\n",
        "    #  STEP 3: can edit HYPERPARAMETERS: for decision tree variables\n",
        "    dt_max_depths = [50, 100, 200, 400, None]\n",
        "    dt_min_samples_splits = [2, 10, 20, 50]\n",
        "\n",
        "    # grid searching model results for decision tree on all types of data with all types of inputs\n",
        "    for scalar_type in tqdm(data_testing.keys(), desc=\"decision tree scaled vs original\"):\n",
        "        if scalar_type == 'columns':\n",
        "            continue\n",
        "        X_test = data_testing[scalar_type][\"X_test\"]\n",
        "        X_train = data_testing[scalar_type][\"X_train\"]\n",
        "        y_train = data_testing[scalar_type][\"y_train\"]\n",
        "        y_test = data_testing[scalar_type][\"y_test\"]\n",
        "        feature_names = data_testing[\"columns\"].tolist()\n",
        "\n",
        "        # going through possible decision tree combos\n",
        "        for dt_min_samples_split in dt_min_samples_splits:\n",
        "            for dt_max_depth in dt_max_depths:\n",
        "                # modeling portion\n",
        "                model = DecisionTreeClassifier(\n",
        "                    max_depth=dt_max_depth, min_samples_split=dt_min_samples_split\n",
        "                )\n",
        "                model.fit(X_train, y_train)\n",
        "                y_pred = model.predict(X_test)\n",
        "                y_pred_proba = model.predict_proba(X_test)[:, 1] # Probability of the positive class\n",
        "\n",
        "                # validation\n",
        "                accuracy = accuracy_score(y_test, y_pred)\n",
        "                precision = precision_score(y_test, y_pred, zero_division=0)\n",
        "                recall = recall_score(y_test, y_pred, zero_division=0)\n",
        "                f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "                conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "                roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "                pr_auc = average_precision_score(y_test, y_pred_proba)\n",
        "\n",
        "                # getting feature importance\n",
        "                tree_imp = model.feature_importances_\n",
        "                feature_imp = dict(zip(feature_names, tree_imp))\n",
        "                feature_imp_json = json.dumps({k: float(v) for k, v in feature_imp.items()})\n",
        "\n",
        "                # getting permutation importance\n",
        "                perm = permutation_importance(\n",
        "                    model, X_test, y_test,\n",
        "                    n_repeats=10,\n",
        "                    random_state=RANDOM_STATE,\n",
        "                    scoring=\"f1\"  # or \"accuracy\"\n",
        "                )\n",
        "\n",
        "                perm_imp = dict(zip(feature_names, perm.importances_mean))\n",
        "                perm_imp_json = json.dumps({k: float(v) for k, v in perm_imp.items()})\n",
        "\n",
        "                # STEP 3, add any hyperparameters to each of these results/outputs\n",
        "                dec_tree_results.append(\n",
        "                    {\n",
        "                        \"file name\": file_info.name,\n",
        "                        \"accuracy\": accuracy,\n",
        "                        \"precision\": precision,\n",
        "                        \"recall\": recall,\n",
        "                        \"f1\": f1,\n",
        "                        \"roc_auc\": roc_auc,\n",
        "                        \"pr_auc\": pr_auc,\n",
        "                        \"confusion matrix\": conf_matrix,\n",
        "                        \"test size\": TEST_SIZE,\n",
        "                        \"random state\": RANDOM_STATE,\n",
        "                        \"scalar_status\": scalar_type,\n",
        "                        \"y variable used\": Y_COL,\n",
        "                        \"model used\": \"decision_tree\",\n",
        "                        \"logistic_reg_c\": np.nan,\n",
        "                        \"lr_ratios\": np.nan,\n",
        "                        \"nn_layers\": np.nan,\n",
        "                        \"nn_neurons\": np.nan,\n",
        "                        \"nn_batch_size\": np.nan,\n",
        "                        \"nn_epochs\": np.nan,\n",
        "                        \"dt_max_depth\": dt_max_depth,\n",
        "                        \"dt_min_samples_split\": dt_min_samples_split,\n",
        "                        \"svm_c_val\": np.nan,\n",
        "                        \"svm_kernel\": np.nan,\n",
        "                        \"knn_n_neighbor\": np.nan,\n",
        "                        \"knn_weights\": np.nan,\n",
        "                        \"gbm_learning_rate\": np.nan,\n",
        "                        \"gbm_n_estimator\": np.nan,\n",
        "                        \"rf_n_estimators\": np.nan,\n",
        "                        \"rf_max_depth\": np.nan,\n",
        "                        \"rf_min_samples_leaf\": np.nan,\n",
        "                        \"coefficient_importance\": feature_imp_json,\n",
        "                        \"permutation_importance\": perm_imp_json,\n",
        "                        \"y_pred_proba\": y_pred_proba.tolist(),\n",
        "                    }\n",
        "                )\n",
        "\n",
        "                # Store y_test and y_pred_proba for PR/AUC plotting\n",
        "                model_predictions_data.append({\n",
        "                    \"file name\": file_info.name,\n",
        "                    \"model used\": \"decision_tree\",\n",
        "                    \"scalar_status\": scalar_type,\n",
        "                    \"dt_max_depth\": dt_max_depth,\n",
        "                    \"dt_min_samples_split\": dt_min_samples_split,\n",
        "                    \"y_test\": y_test.tolist(),\n",
        "                    \"y_pred_proba\": y_pred_proba.tolist()\n",
        "                })\n",
        "\n",
        "    return dec_tree_results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_random_forest(data_testing, file_info, model_predictions_data):\n",
        "    \"\"\"\n",
        "    ----- inputs -----\n",
        "    data_testing: dict[str=scalingType][str=y/X train/test label][pd.DataFrame],\n",
        "        dictionary contianing,\n",
        "            * string of scaling type (standard scalar, orig),\n",
        "                * string of what dataset grabbing (X_train, X_test, y_train, y_test),\n",
        "                    * corresponding data in a pandas dataframe,\n",
        "    model_predictions_data: list\n",
        "        A list to store dictionaries of y_true and y_pred_proba for PR/AUC curves.\n",
        "    ----- outputs -----\n",
        "    \"\"\"\n",
        "    # results table\n",
        "    rf_results = []\n",
        "\n",
        "    # STEP 3: edit HYPERPARAMETERS for random forest\n",
        "    # Expanded hyperparameter search space\n",
        "    rf_n_estimators = [100, 300, 500]\n",
        "    rf_max_depths = [None, 10, 50]\n",
        "    rf_min_samples_leaf = [1, 2, 4]\n",
        "    #rf_min_samples_split = [2, 5, 10]\n",
        "\n",
        "    for scalar_type in tqdm(data_testing.keys(), desc=\"random forest scaled vs original\"):\n",
        "        if scalar_type == 'columns':\n",
        "            continue\n",
        "        X_test = data_testing[scalar_type][\"X_test\"]\n",
        "        X_train = data_testing[scalar_type][\"X_train\"]\n",
        "        y_train = data_testing[scalar_type][\"y_train\"]\n",
        "        y_test = data_testing[scalar_type][\"y_test\"]\n",
        "        feature_names = data_testing[\"columns\"].tolist()\n",
        "\n",
        "        # Diagnostic prints (optional, can be removed once issue is resolved)\n",
        "        print(f\"\\n--- Diagnosing {scalar_type} for Random Forest ---\")\n",
        "        print(f\"X_train shape: {X_train.shape}\")\n",
        "        print(f\"y_train shape: {y_train.shape}\")\n",
        "        print(f\"X_test shape: {X_test.shape}\")\n",
        "        print(f\"y_test shape: {y_test.shape}\")\n",
        "\n",
        "        for rf_n_estimator in rf_n_estimators:\n",
        "            for rf_max_depth in rf_max_depths:\n",
        "                for rf_min_leaf in rf_min_samples_leaf:\n",
        "                        model = RandomForestClassifier(\n",
        "                                n_estimators=rf_n_estimator,\n",
        "                                max_depth=rf_max_depth,\n",
        "                                min_samples_leaf=rf_min_leaf,\n",
        "                                random_state=RANDOM_STATE,\n",
        "                                n_jobs=-1\n",
        "                        )\n",
        "                        model.fit(X_train, y_train)\n",
        "                        y_pred = model.predict(X_test)\n",
        "                        y_pred_proba = model.predict_proba(X_test)[:, 1] # Probability of the positive class\n",
        "\n",
        "                        # validation\n",
        "                        accuracy = accuracy_score(y_test, y_pred)\n",
        "                        precision = precision_score(y_test, y_pred, zero_division=0)\n",
        "                        recall = recall_score(y_test, y_pred, zero_division=0)\n",
        "                        f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "                        conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "                        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "                        pr_auc = average_precision_score(y_test, y_pred_proba)\n",
        "\n",
        "                        # getting feature importance\n",
        "                        rf_imp = model.feature_importances_\n",
        "                        feature_imp = dict(zip(feature_names, rf_imp))\n",
        "                        feature_imp_json = json.dumps({k: float(v) for k, v in feature_imp.items()})\n",
        "\n",
        "                        # getting permutation importance\n",
        "                        perm = permutation_importance(\n",
        "                          model, X_test, y_test,\n",
        "                          n_repeats=3,\n",
        "                          random_state=RANDOM_STATE,\n",
        "                          scoring=\"f1\"  # or \"accuracy\"\n",
        "                        )\n",
        "\n",
        "                        perm_imp = dict(zip(feature_names, perm.importances_mean))\n",
        "                        perm_imp_json = json.dumps({k: float(v) for k, v in perm_imp.items()})\n",
        "\n",
        "                        # STEP 3, add any hyperparameters to each of these results/outputs\n",
        "                        rf_results.append(\n",
        "                            {\n",
        "                                \"file name\": file_info.name,\n",
        "                                \"accuracy\": accuracy,\n",
        "                                \"precision\": precision,\n",
        "                                \"recall\": recall,\n",
        "                                \"f1\": f1,\n",
        "                                \"roc_auc\": roc_auc,\n",
        "                                \"pr_auc\": pr_auc,\n",
        "                                \"confusion matrix\": conf_matrix,\n",
        "                                \"test size\": TEST_SIZE,\n",
        "                                \"random state\": RANDOM_STATE,\n",
        "                                \"scalar_status\": scalar_type,\n",
        "                                \"y variable used\": Y_COL,\n",
        "                                \"model used\": \"random_forest\",\n",
        "                                \"logistic_reg_c\": np.nan,\n",
        "                                \"lr_ratios\": np.nan,\n",
        "                                \"nn_layers\": np.nan,\n",
        "                                \"nn_neurons\": np.nan,\n",
        "                                \"nn_batch_size\": np.nan,\n",
        "                                \"nn_epochs\": np.nan,\n",
        "                                \"dt_max_depth\": np.nan,\n",
        "                                \"dt_min_samples_split\": np.nan,\n",
        "                                \"svm_c_val\": np.nan,\n",
        "                                \"svm_kernel\": np.nan,\n",
        "                                \"knn_n_neighbor\": np.nan,\n",
        "                                \"knn_weights\": np.nan,\n",
        "                                \"gbm_learning_rate\": np.nan,\n",
        "                                \"gbm_n_estimator\": np.nan,\n",
        "                                \"rf_n_estimators\": rf_n_estimator,\n",
        "                                \"rf_max_depth\": rf_max_depth,\n",
        "                                \"rf_min_samples_leaf\": rf_min_leaf,\n",
        "                                \"coefficient_importance\": feature_imp_json,\n",
        "                                \"permutation_importance\": perm_imp_json,\n",
        "                                \"y_pred_proba\": y_pred_proba.tolist(),\n",
        "                            }\n",
        "                        )\n",
        "\n",
        "                        # Store y_test and y_pred_proba for PR/AUC plotting\n",
        "                        model_predictions_data.append({\n",
        "                            \"file name\": file_info.name,\n",
        "                            \"model used\": \"random_forest\",\n",
        "                            \"scalar_status\": scalar_type,\n",
        "                            \"rf_n_estimators\": rf_n_estimator,\n",
        "                            \"rf_max_depth\": rf_max_depth,\n",
        "                            \"rf_min_samples_leaf\": rf_min_leaf,\n",
        "                            \"y_test\": y_test.tolist(),\n",
        "                            \"y_pred_proba\": y_pred_proba.tolist()\n",
        "                        })\n",
        "\n",
        "    return rf_results"
      ],
      "metadata": {
        "id": "swZl1xN2QiHS"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjtgjul0cTBP"
      },
      "source": [
        "# Prepping the models to Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "tudxNRjiSMrV"
      },
      "outputs": [],
      "source": [
        "def run_models_for_file(file_info, model_predictions_data) -> list:\n",
        "    \"\"\"\n",
        "    Goal: return model results for file\n",
        "\n",
        "    Paramaters:\n",
        "        file: str\n",
        "            the name of the file want to model\n",
        "        file_info: Path object\n",
        "            parsed information about the file\n",
        "        model_predictions_data: list\n",
        "            A list to store dictionaries of y_true and y_pred_proba for PR/AUC curves.\n",
        "\n",
        "    Outputs:\n",
        "        all_rows: list\n",
        "            list of the dictionary model results\n",
        "    \"\"\"\n",
        "\n",
        "    df = data_prep(file_info)\n",
        "    if df.empty:\n",
        "        return []\n",
        "\n",
        "    data_testing = get_train_test(df, y_col=Y_COL, scaling_used=True)\n",
        "\n",
        "    # STEP 5: Choosing which model results to run (comment or uncomment models as needed)\n",
        "    # STEP 4: Add model name here if needed\n",
        "    model_fns = [\n",
        "          #test_logistic_reg,\n",
        "          #test_neural_net,\n",
        "          #test_knn,\n",
        "          #test_decision_tree,\n",
        "          test_random_forest,\n",
        "          #test_svm,\n",
        "          #test_gbm,\n",
        "    ]\n",
        "\n",
        "    # running each model in the model funcs list to return the results\n",
        "    all_rows = []\n",
        "    for fn in model_fns:\n",
        "        rows = fn(data_testing, file_info, model_predictions_data) # running each function\n",
        "        if rows:\n",
        "            all_rows.extend(rows)\n",
        "\n",
        "    return all_rows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRHGz4nqTzmI"
      },
      "source": [
        "# Running all Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOaqObWPShdS",
        "outputId": "08b57bd4-5d52-4ec1-fc6e-ebf02a34006c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rrandom forest scaled vs original:   0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Diagnosing standard_scalar for Random Forest ---\n",
            "X_train shape: (485, 34)\n",
            "y_train shape: (485,)\n",
            "X_test shape: (137, 34)\n",
            "y_test shape: (137,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rrandom forest scaled vs original:  67%|   | 2/3 [05:04<02:32, 152.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Diagnosing orig for Random Forest ---\n",
            "X_train shape: (485, 34)\n",
            "y_train shape: (485,)\n",
            "X_test shape: (137, 34)\n",
            "y_test shape: (137,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "random forest scaled vs original: 100%|| 3/3 [10:09<00:00, 203.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "COMPLETED: k\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Initialize a list to store y_test and y_pred_proba for PR/AUC plotting\n",
        "model_predictions_data = []\n",
        "\n",
        "# running each model to get results\n",
        "rows_results = run_models_for_file(file_info, model_predictions_data) # calling function to run models\n",
        "dataframe_rows_results = pd.DataFrame(rows_results) # converting into a dataframe, so that we can save it\n",
        "dataframe_rows_results.to_csv(f'results_for_{file_info.name}') # saving it into the files section as a CSV\n",
        "print(\"\\n\\nCOMPLETED: k\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urXOEanLT15M"
      },
      "source": [
        "# Analyzing Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnIy0KWQjesW"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import plotly.graph_objects as go\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import json\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPNw7YycrSC9"
      },
      "outputs": [],
      "source": [
        "# STEP 1 for analyze results global variables!\n",
        "results_df = pd.read_csv(f\"results_for_{file_info.name}\")\n",
        "\n",
        "hyperparam_cols = [\n",
        "    \"logistic_reg_c\", \"lr_ratios\",\n",
        "    \"nn_layers\", \"nn_neurons\", \"nn_batch_size\", \"nn_epochs\",\n",
        "    \"dt_max_depth\", \"dt_min_samples_split\",\n",
        "    \"svm_c_val\", \"svm_kernel\",\n",
        "    \"knn_n_neighbor\", \"knn_metric\",\n",
        "    \"gbm_learning_rate\", \"gbm_n_estimator\",\n",
        "    \"rf_n_estimators\", \"rf_max_depth\", \"rf_min_samples_leaf\"\n",
        "]\n",
        "\n",
        "# Base grouping keys\n",
        "base_cols = [\"model used\", \"scalar_status\"]\n",
        "# DO NOT CHANGE THE LINE BELOW: This defines a \"hyperparam combo\"\n",
        "combo_cols = base_cols + hyperparam_cols\n",
        "\n",
        "# whatever you want your graphs to be looking at\n",
        "primary_metric = 'accuracy'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sgVq5PSrM6u"
      },
      "source": [
        "## Helper functions for analyzing results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B42ibx4_rLC4"
      },
      "outputs": [],
      "source": [
        "def parse_confusion_matrix(cm_str):\n",
        "    \"\"\"\n",
        "    Parses strings like:\n",
        "      '[[0 7]\\n [0 8]]'\n",
        "      '[[7 0]\\n [8 0]]'\n",
        "    Returns tn, fp, fn, tp as ints, or NaNs if missing.\n",
        "    \"\"\"\n",
        "    if pd.isna(cm_str):\n",
        "        return np.nan, np.nan, np.nan, np.nan\n",
        "\n",
        "    s = str(cm_str).strip()\n",
        "\n",
        "    # extract 4 integers in row-major order\n",
        "    nums = re.findall(r\"-?\\d+\", s)\n",
        "    if len(nums) < 4:\n",
        "        return np.nan, np.nan, np.nan, np.nan\n",
        "\n",
        "    tn, fp, fn, tp = map(int, nums[:4])\n",
        "    return tn, fp, fn, tp\n",
        "\n",
        "def safe_div(a, b):\n",
        "    return np.where(b == 0, np.nan, a / b)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add_derived_columns(df):\n",
        "    # Parse confusion matrix\n",
        "    tn_fp_fn_tp = df[\"confusion matrix\"].apply(parse_confusion_matrix)\n",
        "    df[[\"tn\", \"fp\", \"fn\", \"tp\"]] = pd.DataFrame(tn_fp_fn_tp.tolist(), index=df.index)\n",
        "\n",
        "    # test n (prefer your stored test size; otherwise derive from cm)\n",
        "    df[\"n_test_cm\"] = df[[\"tn\", \"fp\", \"fn\", \"tp\"]].sum(axis=1)\n",
        "    df[\"n_test\"] = df[\"n_test_cm\"] #This line was edited to fix the proportion of predicted positive rate\n",
        "\n",
        "    # getting rate metrics!\n",
        "    df[\"tpr_recall\"] = safe_div(df[\"tp\"], (df[\"tp\"] + df[\"fn\"]))         # same as recall\n",
        "    df[\"tnr_specificity\"] = safe_div(df[\"tn\"], (df[\"tn\"] + df[\"fp\"]))\n",
        "    df[\"fpr\"] = safe_div(df[\"fp\"], (df[\"fp\"] + df[\"tn\"]))\n",
        "    df[\"pred_pos_rate\"] = safe_div((df[\"tp\"] + df[\"fp\"]), df[\"n_test\"])\n",
        "    df[\"prevalence\"] = safe_div((df[\"tp\"] + df[\"fn\"]), df[\"n_test\"])\n",
        "\n",
        "    df[\"balanced_accuracy\"] = 0.5 * (df[\"tpr_recall\"] + df[\"tnr_specificity\"])\n",
        "\n",
        "    # Degenerate predictor flags\n",
        "    df[\"degenerate_all_pos\"] = (df[\"tn\"] == 0) & (df[\"fn\"] == 0) & df[\"n_test\"].notna()\n",
        "    df[\"degenerate_all_neg\"] = (df[\"tp\"] == 0) & (df[\"fp\"] == 0) & df[\"n_test\"].notna()\n",
        "\n",
        "    # clean model name\n",
        "    df[\"model used\"] = df[\"model used\"].astype(str).str.strip().str.lower()\n",
        "    return df\n",
        "\n",
        "# Immediately adding the parsed cols\n",
        "results_df = add_derived_columns(results_df)"
      ],
      "metadata": {
        "id": "9AN0IxgFpArj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_stats = results_df.groupby(['model used', 'scalar_status'])[['accuracy', 'precision', 'recall', 'f1']].mean()\n",
        "print(\"Summary Statistics of Performance Metrics for Each Model:\")\n",
        "print(summary_stats)"
      ],
      "metadata": {
        "id": "N9PdJucO0UGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Container for the best probabilities\n",
        "best_model_probabilities = {}\n",
        "\n",
        "# Ensure the column exists\n",
        "if 'y_pred_proba' not in results_df.columns:\n",
        "    print(\"Error: 'y_pred_proba' column not found in results_df. Please re-run the models to generate probabilities.\")\n",
        "else:\n",
        "    print(f\"Extracting probabilities for best models based on {primary_metric}...\")\n",
        "\n",
        "    # 1. Identify best models\n",
        "    # Group by model type and scalar status, then find the row with max primary_metric\n",
        "    # We use .reset_index() to keep the index accessible\n",
        "    best_indices = results_df.groupby(['model used', 'scalar_status'])[primary_metric].idxmax()\n",
        "    best_rows = results_df.loc[best_indices]\n",
        "\n",
        "    for index, row in best_rows.iterrows():\n",
        "        model_name = row['model used']\n",
        "        scalar = row['scalar_status']\n",
        "        score = row[primary_metric]\n",
        "\n",
        "        # 2. Extract and parse probabilities\n",
        "        prob_str = row.get('y_pred_proba')\n",
        "\n",
        "        if pd.isna(prob_str):\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # If loaded from CSV, it's a string representation of a list\n",
        "            if isinstance(prob_str, str):\n",
        "                probs = json.loads(prob_str)\n",
        "            else:\n",
        "                # If the dataframe is fresh from memory, it might already be a list\n",
        "                probs = prob_str\n",
        "\n",
        "            key = f\"{model_name} ({scalar})\"\n",
        "            best_model_probabilities[key] = probs\n",
        "            print(f\"Stored probs for {key:<30} (Score: {score:.4f})\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing probabilities for {model_name}: {e}\")\n",
        "\n",
        "# Example usage:\n",
        "# probs = best_model_probabilities.get('random_forest (orig)')"
      ],
      "metadata": {
        "id": "J3vdRS4PeCC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yut9HkxlpAYA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "He7wERBrsCaJ"
      },
      "source": [
        "## Graphing Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUPBUW3AukQs"
      },
      "outputs": [],
      "source": [
        "tmp = results_df.dropna(subset=[\"pred_pos_rate\", primary_metric]).copy()\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.scatterplot(\n",
        "    data=tmp,\n",
        "    x=\"pred_pos_rate\",\n",
        "    y=primary_metric,\n",
        "    hue=\"model used\",\n",
        "    style=\"scalar_status\" if \"scalar_status\" in tmp.columns else None,\n",
        "    alpha=0.75,\n",
        ")\n",
        "plt.axvline(0.0, linestyle=\"--\")\n",
        "plt.axvline(1.0, linestyle=\"--\")\n",
        "plt.title(f\"Degenerate check: predicted positive rate vs {primary_metric}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_feature_importances(df, model_name, importance_col='permutation_importance', top_n=12):\n",
        "    \"\"\"\n",
        "    Aggregates and plots mean feature importance for a given model.\n",
        "    importance_col: 'permutation_importance' or 'coefficient_importance'.\n",
        "    \"\"\"\n",
        "    # Filter the DataFrame for the specific model and where the importance column is not NaN\n",
        "    subset = df[(df['model used'] == model_name) & df[importance_col].notna()]\n",
        "    # No data to plot if the subset is empty\n",
        "    if subset.empty:\n",
        "        return\n",
        "\n",
        "    rows = []\n",
        "    # Iterate through each row in the filtered subset\n",
        "    for _, row in subset.iterrows():\n",
        "        # Parsing the importance string (which is JSON) into a dictionary\n",
        "        try:\n",
        "            val_str = row[importance_col]\n",
        "            if pd.isna(val_str): continue\n",
        "            imp_dict = json.loads(str(val_str))\n",
        "        except Exception:\n",
        "            continue\n",
        "        # For each feature and its value in the importance dictionary\n",
        "        for feat, val in imp_dict.items():\n",
        "            # Append a dictionary with the feature and its importance to the rows list\n",
        "            rows.append({'feature': feat, 'importance': val})\n",
        "\n",
        "    if not rows:\n",
        "        return\n",
        "\n",
        "    # Create a DataFrame from the extracted feature importances\n",
        "    # Group by feature, calculate the mean importance\n",
        "    agg_df = pd.DataFrame(rows).groupby('feature')['importance'].mean()\n",
        "\n",
        "    # Sort by absolute magnitude to capture strong negative coefficients\n",
        "    agg_df_abs = agg_df.abs().sort_values(ascending=False).head(top_n)\n",
        "\n",
        "    # Select the original values for those top N features (to show sign)\n",
        "    imp_df = agg_df.loc[agg_df_abs.index]\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.barplot(x=imp_df.values, y=imp_df.index, hue=imp_df.index, palette='viridis', legend=False)\n",
        "    plt.title(f\"{model_name}  top {top_n} {importance_col.replace('_', ' ')} (mean)\")\n",
        "    plt.xlabel('Mean importance')\n",
        "    plt.ylabel('Feature')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Generate plots for each model present in the results_df\n",
        "models = results_df['model used'].dropna().unique() if 'model used' in results_df.columns else []\n",
        "for m in models:\n",
        "    # If 'permutation_importance' column exists, plot permutation importances\n",
        "    if 'permutation_importance' in results_df.columns:\n",
        "        plot_feature_importances(results_df, m, 'permutation_importance', top_n=12)\n",
        "    # If 'coefficient_importance' column exists, plot coefficient importances\n",
        "    if 'coefficient_importance' in results_df.columns:\n",
        "        plot_feature_importances(results_df, m, 'coefficient_importance', top_n=12)"
      ],
      "metadata": {
        "id": "W8HlxNhmfMd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d50b770"
      },
      "source": [
        "## AUC and Precision-Recall Curves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b256340b"
      },
      "source": [
        "# Helper function to plot PR curve\n",
        "def plot_pr_curve(y_test, y_pred_proba, title):\n",
        "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
        "    # Explicitly create figure and axes\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    # Pass the axes to the display plotter\n",
        "    PrecisionRecallDisplay(precision=precision, recall=recall).plot(ax=ax)\n",
        "    ax.set_title(title)\n",
        "    ax.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Find the best model for each type based on primary_metric\n",
        "best_models = results_df.loc[results_df.groupby(['model used', 'scalar_status'])[primary_metric].idxmax()]\n",
        "\n",
        "# Loop through each stored prediction and plot PR curve\n",
        "for pred_data in model_predictions_data:\n",
        "    model_name = pred_data['model used']\n",
        "    scalar_status = pred_data['scalar_status']\n",
        "\n",
        "    # Get the row from best_models that matches current pred_data's model and scalar_status\n",
        "    current_best_model = best_models[\n",
        "        (best_models['model used'] == model_name) &\n",
        "        (best_models['scalar_status'] == scalar_status)\n",
        "    ]\n",
        "\n",
        "    if not current_best_model.empty:\n",
        "        # Check if this specific prediction data matches the best model's hyperparameters\n",
        "        # This requires reconstructing the hyperparameter combination for comparison\n",
        "        is_best = False\n",
        "\n",
        "        # Helper to safely compare values including NaNs and None\n",
        "        def is_match(val1, val2):\n",
        "            s1, s2 = str(val1), str(val2)\n",
        "            # Treat 'nan' and 'None' as equivalent for comparison\n",
        "            if s1 in ['nan', 'None'] and s2 in ['nan', 'None']: return True\n",
        "            try:\n",
        "                return np.isclose(float(val1), float(val2))\n",
        "            except:\n",
        "                return s1 == s2\n",
        "\n",
        "        row = current_best_model.iloc[0]\n",
        "\n",
        "        if model_name == \"logistic regression\":\n",
        "            if is_match(row['logistic_reg_c'], pred_data['logistic_reg_c']) and \\\n",
        "               is_match(row['lr_ratios'], pred_data['lr_ratios']):\n",
        "                is_best = True\n",
        "        elif model_name == \"neural net\":\n",
        "            if is_match(row['nn_layers'], pred_data['nn_layers']) and \\\n",
        "               is_match(row['nn_neurons'], pred_data['nn_neurons']) and \\\n",
        "               is_match(row['nn_batch_size'], pred_data['nn_batch_size']) and \\\n",
        "               is_match(row['nn_epochs'], pred_data['nn_epochs']):\n",
        "                is_best = True\n",
        "        elif model_name == \"knn\":\n",
        "            if is_match(row['knn_n_neighbor'], pred_data['knn_n_neighbor']) and \\\n",
        "               is_match(row['knn_weights'], pred_data['knn_weights']):\n",
        "                is_best = True\n",
        "        elif model_name == \"decision_tree\":\n",
        "            if is_match(row['dt_max_depth'], pred_data['dt_max_depth']) and \\\n",
        "               is_match(row['dt_min_samples_split'], pred_data['dt_min_samples_split']):\n",
        "                is_best = True\n",
        "        elif model_name == \"random_forest\":\n",
        "            if is_match(row['rf_n_estimators'], pred_data['rf_n_estimators']) and \\\n",
        "               is_match(row['rf_max_depth'], pred_data['rf_max_depth']) and \\\n",
        "               is_match(row['rf_min_samples_leaf'], pred_data['rf_min_samples_leaf']) and \\\n",
        "               is_match(row['rf_min_samples_split'], pred_data['rf_min_samples_split']):\n",
        "                is_best = True\n",
        "        elif model_name == \"svm\":\n",
        "            if is_match(row['svm_c_val'], pred_data['svm_c_val']) and \\\n",
        "               is_match(row['svm_kernel'], pred_data['svm_kernel']):\n",
        "                is_best = True\n",
        "        elif model_name == \"gbm\":\n",
        "            if is_match(row['gbm_learning_rate'], pred_data['gbm_learning_rate']) and \\\n",
        "               is_match(row['gbm_n_estimator'], pred_data['gbm_n_estimator']):\n",
        "                is_best = True\n",
        "        # If no specific hyperparameters are relevant for the model (e.g. if it's the only one of its type run, or no hyperparameters were varied),\n",
        "        # or if it's the best by comparison above, then proceed.\n",
        "        elif len(current_best_model) == 1: # If only one entry for this model type/scalar status, it must be the best\n",
        "            is_best = True\n",
        "\n",
        "        if is_best:\n",
        "            y_test = np.array(pred_data['y_test'])\n",
        "            y_pred_proba = np.array(pred_data['y_pred_proba'])\n",
        "\n",
        "            # Construct title dynamically based on model and hyperparameters\n",
        "            title = f\"\"\"PR Curve - {model_name} ({scalar_status})\n",
        "Best {primary_metric}: {row[primary_metric]:.4f}\"\"\"\n",
        "\n",
        "            if model_name == \"logistic regression\":\n",
        "                title += f\"\\nC={pred_data['logistic_reg_c']} L1_ratio={pred_data['lr_ratios']}\"\n",
        "            elif model_name == \"neural net\":\n",
        "                title += f\"\\nLayers={pred_data['nn_layers']} Neurons={pred_data['nn_neurons']} Batch={pred_data['nn_batch_size']} Epochs={pred_data['nn_epochs']}\"\n",
        "            elif model_name == \"knn\":\n",
        "                title += f\"\\nNeighbors={pred_data['knn_n_neighbor']} Weights={pred_data['knn_weights']}\"\n",
        "            elif model_name == \"decision_tree\":\n",
        "                title += f\"\\nMaxDepth={pred_data['dt_max_depth']} MinSplit={pred_data['dt_min_samples_split']}\"\n",
        "            elif model_name == \"random_forest\":\n",
        "                title += f\"\\nEstimators={pred_data['rf_n_estimators']} MaxDepth={pred_data['rf_max_depth']} MinLeaf={pred_data['rf_min_samples_leaf']}\"\n",
        "            elif model_name == \"svm\":\n",
        "                title += f\"\\nC={pred_data['svm_c_val']} Kernel={pred_data['svm_kernel']}\"\n",
        "            elif model_name == \"gbm\":\n",
        "                title += f\"\\nLR={pred_data['gbm_learning_rate']} N_Estimators={pred_data['gbm_n_estimator']}\"\n",
        "\n",
        "            plot_pr_curve(y_test, y_pred_proba, title)\n",
        "\n",
        "            # Calculate and print AUC\n",
        "            auc_score = roc_auc_score(y_test, y_pred_proba)\n",
        "            print(f\"{title} - AUC: {auc_score:.4f}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test-train Validation Curves"
      ],
      "metadata": {
        "id": "h9ZRe9EMprcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading current data\n",
        "df_curr = data_prep(file_info)\n",
        "data_testing_curr = get_train_test(df_curr, y_col=Y_COL, scaling_used=True)\n",
        "\n",
        "# Define Training Split Sizes (10% to 100% of training data)\n",
        "train_fractions = np.linspace(0.1, 1.0, 10)\n",
        "\n",
        "def get_best_estimator(row):\n",
        "    \"\"\"Reconstructs the model object from the best results row.\"\"\"\n",
        "    name = row['model used']\n",
        "\n",
        "    # Helper to clean params (handle NaNs)\n",
        "    def get_param(col, default=None, dtype=float):\n",
        "        val = row.get(col)\n",
        "        if pd.isna(val):\n",
        "            return default\n",
        "        if dtype == int:\n",
        "            return int(val)\n",
        "        return val\n",
        "\n",
        "    if name == 'logistic regression':\n",
        "        return LogisticRegression(\n",
        "            C=get_param('logistic_reg_c', 1.0),\n",
        "            l1_ratio=get_param('lr_ratios', 0.0),\n",
        "            max_iter=1000\n",
        "        )\n",
        "    elif name == 'knn':\n",
        "        return KNeighborsClassifier(\n",
        "            n_neighbors=get_param('knn_n_neighbor', 5, int),\n",
        "            weights=row.get('knn_weights', 'uniform')\n",
        "        )\n",
        "    elif name == 'decision_tree':\n",
        "        return DecisionTreeClassifier(\n",
        "            max_depth=get_param('dt_max_depth', None, int),\n",
        "            min_samples_split=get_param('dt_min_samples_split', 2, int),\n",
        "            random_state=RANDOM_STATE\n",
        "        )\n",
        "    elif name == 'random_forest':\n",
        "        return RandomForestClassifier(\n",
        "            n_estimators=get_param('rf_n_estimators', 100, int),\n",
        "            max_depth=get_param('rf_max_depth', None, int),\n",
        "            min_samples_leaf=get_param('rf_min_samples_leaf', 1, int),\n",
        "            min_samples_split=get_param('rf_min_samples_split', 2, int),\n",
        "            random_state=RANDOM_STATE\n",
        "        )\n",
        "    elif name == 'svm':\n",
        "        return SVC(\n",
        "            C=get_param('svm_c_val', 1.0),\n",
        "            kernel=row.get('svm_kernel', 'rbf'),\n",
        "            probability=True,\n",
        "            max_iter=20000\n",
        "        )\n",
        "    elif name == 'gbm':\n",
        "        return GradientBoostingClassifier(\n",
        "            learning_rate=get_param('gbm_learning_rate', 0.1),\n",
        "            n_estimators=get_param('gbm_n_estimator', 100, int),\n",
        "            random_state=RANDOM_STATE\n",
        "        )\n",
        "    elif name == 'neural net':\n",
        "        # Keras models need to be built freshly\n",
        "        def build_nn():\n",
        "            model = Sequential()\n",
        "            model.add(Input(shape=(data_testing_curr['orig']['X_train'].shape[1],)))\n",
        "            layers = get_param('nn_layers', 1, int)\n",
        "            neurons = get_param('nn_neurons', 32, int)\n",
        "            for _ in range(layers):\n",
        "                model.add(Dense(neurons, activation=\"relu\"))\n",
        "            model.add(Dense(1, activation=\"sigmoid\"))\n",
        "            model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "            return model\n",
        "        return build_nn()\n",
        "    return None\n",
        "\n",
        "# Loop through each best model and plot\n",
        "for index, row in best_models.iterrows():\n",
        "    model_name = row['model used']\n",
        "    scalar = row['scalar_status']\n",
        "\n",
        "    # Skip if scalar type doesn't match our data keys (e.g. if we added new scalar types)\n",
        "    if scalar not in data_testing_curr:\n",
        "        continue\n",
        "\n",
        "    X_train_full = data_testing_curr[scalar]['X_train']\n",
        "    y_train_full = data_testing_curr[scalar]['y_train']\n",
        "    X_test = data_testing_curr[scalar]['X_test']\n",
        "    y_test = data_testing_curr[scalar]['y_test']\n",
        "\n",
        "    estimator = get_best_estimator(row)\n",
        "    if estimator is None:\n",
        "        continue\n",
        "\n",
        "    train_scores = []\n",
        "    test_scores = []\n",
        "\n",
        "    # Iterate over subset sizes\n",
        "    for frac in train_fractions:\n",
        "        # Determine subset size\n",
        "        # We maintain order to keep it deterministic-ish or use shuffle=False if strict slice needed\n",
        "        # But here train_test_split is easier to get random subset\n",
        "        if frac < 1.0:\n",
        "            X_sub, _, y_sub, _ = train_test_split(\n",
        "                X_train_full, y_train_full,\n",
        "                train_size=frac,\n",
        "                random_state=RANDOM_STATE,\n",
        "                stratify=y_train_full # Keep class balance\n",
        "            )\n",
        "        else:\n",
        "            X_sub, y_sub = X_train_full, y_train_full\n",
        "\n",
        "        # Train and Evaluate\n",
        "        if model_name == 'neural net':\n",
        "            # Keras specific API\n",
        "            bs = int(row.get('nn_batch_size', 32))\n",
        "            ep = int(row.get('nn_epochs', 10))\n",
        "            estimator.fit(X_sub, y_sub, epochs=ep, batch_size=bs, verbose=0)\n",
        "            # Predict\n",
        "            train_pred = (estimator.predict(X_sub, verbose=0).flatten() > 0.5).astype(int)\n",
        "            test_pred = (estimator.predict(X_test, verbose=0).flatten() > 0.5).astype(int)\n",
        "\n",
        "            train_scores.append(accuracy_score(y_sub, train_pred))\n",
        "            test_scores.append(accuracy_score(y_test, test_pred))\n",
        "\n",
        "            # Re-build for next iteration to avoid incremental learning unless desired\n",
        "            # For learning curve we usually retrain from scratch\n",
        "            estimator = get_best_estimator(row)\n",
        "\n",
        "        else:\n",
        "            # Scikit-learn API\n",
        "            estimator.fit(X_sub, y_sub)\n",
        "            train_scores.append(accuracy_score(y_sub, estimator.predict(X_sub)))\n",
        "            test_scores.append(accuracy_score(y_test, estimator.predict(X_test)))\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.plot(train_fractions * 100, train_scores, 'o-', label=\"Train Accuracy\", color=\"darkorange\")\n",
        "    plt.plot(train_fractions * 100, test_scores, 'o-', label=\"Test Accuracy (Validation)\", color=\"navy\")\n",
        "    plt.title(f\"Learning Curve: {model_name} ({scalar})\")\n",
        "    plt.xlabel(\"Percentage of Training Data Used\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.ylim(0.0, 1.05)\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "X1zkhlwToXSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving Models"
      ],
      "metadata": {
        "id": "nqMlRzEPuMtA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "Byq33A3euNAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_feature_importances(df, model_name, importance_col='permutation_importance', top_n=12):\n",
        "    \"\"\"\n",
        "    Aggregates and plots mean feature importance for a given model.\n",
        "    importance_col: 'permutation_importance' or 'coefficient_importance'.\n",
        "    \"\"\"\n",
        "    # Filter the DataFrame for the specific model and where the importance column is not NaN\n",
        "    subset = df[(df['model used'] == model_name) & df[importance_col].notna()]\n",
        "    # No data to plot if the subset is empty\n",
        "    if subset.empty:\n",
        "        return\n",
        "\n",
        "    rows = []\n",
        "    # Iterate through each row in the filtered subset\n",
        "    for _, row in subset.iterrows():\n",
        "        # Parsing the importance string (which is JSON) into a dictionary\n",
        "        try:\n",
        "            val_str = row[importance_col]\n",
        "            if pd.isna(val_str): continue\n",
        "            imp_dict = json.loads(str(val_str))\n",
        "        except Exception:\n",
        "            continue\n",
        "        # For each feature and its value in the importance dictionary\n",
        "        for feat, val in imp_dict.items():\n",
        "            # Append a dictionary with the feature and its importance to the rows list\n",
        "            rows.append({'feature': feat, 'importance': val})\n",
        "\n",
        "    if not rows:\n",
        "        return\n",
        "\n",
        "    # Create a DataFrame from the extracted feature importances\n",
        "    # Group by feature, calculate the mean importance\n",
        "    agg_df = pd.DataFrame(rows).groupby('feature')['importance'].mean()\n",
        "\n",
        "    # Sort by absolute magnitude to capture strong negative coefficients\n",
        "    agg_df_abs = agg_df.abs().sort_values(ascending=False).head(top_n)\n",
        "\n",
        "    # Select the original values for those top N features (to show sign)\n",
        "    imp_df = agg_df.loc[agg_df_abs.index]\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.barplot(x=imp_df.values, y=imp_df.index, hue=imp_df.index, palette='viridis', legend=False)\n",
        "    plt.title(f\"{model_name}  top {top_n} {importance_col.replace('_', ' ')} (mean)\")\n",
        "    plt.xlabel('Mean importance')\n",
        "    plt.ylabel('Feature')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Generate plots for each model present in the results_df\n",
        "models = results_df['model used'].dropna().unique() if 'model used' in results_df.columns else []\n",
        "for m in models:\n",
        "    # If 'permutation_importance' column exists, plot permutation importances\n",
        "    if 'permutation_importance' in results_df.columns:\n",
        "        plot_feature_importances(results_df, m, 'permutation_importance', top_n=12)\n",
        "    # If 'coefficient_importance' column exists, plot coefficient importances\n",
        "    if 'coefficient_importance' in results_df.columns:\n",
        "        plot_feature_importances(results_df, m, 'coefficient_importance', top_n=12)"
      ],
      "metadata": {
        "id": "p5fDanDBuTJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  saving gbm model\n",
        "# hyperparams\n",
        "gbm_learning_rate = 0.1\n",
        "gbm_n_estimator = 400\n",
        "scalar_type = \"standard_scalar\"\n",
        "\n",
        "\n",
        "print(\"cwd:\", Path.cwd())\n",
        "print(\"file_info:\", file_info)\n",
        "print(\"exists:\", Path(file_info).exists())\n",
        "df = data_prep(file_info)\n",
        "data_testing = get_train_test(df, y_col=Y_COL, scaling_used=True)\n",
        "\n",
        "save_gbm(data_testing,  scalar_type, gbm_learning_rate, gbm_n_estimator, \"input_model_file.pkl\")"
      ],
      "metadata": {
        "id": "ys2gE78cuUHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soil_vars_only = ['pH', 'Copper (mg/Kg)', 'Molybdenum (mg/Kg)', 'log of Sulfur (mg/Kg)', 'log of Moisture',\n",
        "       'log of Manganese (mg/Kg)', 'log of Aluminum (mg/Kg)', 'log of Potassium (mg/Kg)',\n",
        "       'log of Total nitrogen (%)', 'double log of Zinc (mg/Kg)', 'log of Organic matter (%)', 'log of Phosphorus (mg/Kg)',\n",
        "       'log of Iron (mg/Kg)', 'log of Magnesium (mg/Kg)', 'log of Sodium (mg/Kg)', 'log of Calcium (mg/Kg)', 'scaled_cluster_kmeans', 'cluster_kmeans']\n",
        "long_lat_vars_only = ['Latitude', 'Longitude',\n",
        "       'Precipitation (mm)', 'Max temperature ( )', 'Min temperature ( )',\n",
        "       'Wind speed (m/s)', 'Barren (%)', 'Forest (%)', 'Pasture (%)',\n",
        "       'log of Grassland (%)', 'log of Shrubland (%)',\n",
        "       'log of Open water (%)', 'log of Total carbon (%)',\n",
        "       'log of Developed open space (> 20% Impervious Cover) (%)', 'log of Elevation (m)',\n",
        "        'log of Cropland (%)', 'log of Wetland (%)', \"log of Developed open space (< 20% Impervious Cover) (%)\",\n",
        "       'scaled_cluster_kmeans', 'cluster_kmeans']"
      ],
      "metadata": {
        "id": "oWV3efA8uYHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# only latitude longitude accessible data\n",
        "# (elevation, long/lat, and weather), aka no soil\n",
        "\n",
        "#  saving gbm model\n",
        "# hyperparams\n",
        "gbm_learning_rate = 0.1\n",
        "gbm_n_estimator = 400\n",
        "scalar_type = \"standard_scalar\"\n",
        "\n",
        "\n",
        "print(\"cwd:\", Path.cwd())\n",
        "print(\"file_info:\", file_info)\n",
        "print(\"exists:\", Path(file_info).exists())\n",
        "df = data_prep(file_info)\n",
        "\n",
        "df = df.drop(columns=soil_vars_only) # dropping soil variables to create a long lat only\n",
        "# also drops clusters because those were developed with full variables.\n",
        "data_testing = get_train_test(df, y_col=Y_COL, scaling_used=True)\n",
        "\n",
        "save_gbm(data_testing,  scalar_type, gbm_learning_rate, gbm_n_estimator, \"input_model_file_only_longlat.pkl\")"
      ],
      "metadata": {
        "id": "JYfHdkcouceW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# only soil data\n",
        "# (moisture, soil) aka no weather or long/lat\n",
        "\n",
        "#  saving gbm model\n",
        "# hyperparams\n",
        "gbm_learning_rate = 0.1\n",
        "gbm_n_estimator = 400\n",
        "scalar_type = \"standard_scalar\"\n",
        "\n",
        "\n",
        "print(\"cwd:\", Path.cwd())\n",
        "print(\"file_info:\", file_info)\n",
        "print(\"exists:\", Path(file_info).exists())\n",
        "df = data_prep(file_info)\n",
        "\n",
        "df = df.drop(columns=long_lat_vars_only) # dropping long lat only\n",
        "# also drops clusters because those were developed with full variables.\n",
        "data_testing = get_train_test(df, y_col=Y_COL, scaling_used=True)\n",
        "\n",
        "save_gbm(data_testing,  scalar_type, gbm_learning_rate, gbm_n_estimator, \"input_model_file_only_soil.pkl\")"
      ],
      "metadata": {
        "id": "FvUJf5oeuevy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}